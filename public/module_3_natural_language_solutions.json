{
  "units": [
    {
      "name": "Unit 1: Analyze text with Azure AI Language",
      "questions": [
        {
          "question": "What is the first step to use Azure AI Language service?",
          "options": [
            "Install Microsoft Graph SDK",
            "Provision an Azure AI Language resource",
            "Build a REST client manually",
            "Create a VM"
          ],
          "answer": "Provision an Azure AI Language resource",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ You need to provision the managed Language resource before using any API :contentReference[oaicite:2]{index=2}. Others don’t enable service usage."
        },
        {
          "question": "Which HTTP header is required to call Text Analysis API?",
          "options": [
            "Authorization: Bearer <token>",
            "Ocp-Apim-Subscription-Key",
            "x-ms-request-id",
            "Content-Type: text/plain"
          ],
          "answer": "Ocp-Apim-Subscription-Key",
          "type": "single",
          "difficulty": "hard",
          "explanation": "✅ The REST API requires apim subscription header :contentReference[oaicite:3]{index=3}. 'Bearer' works only with OAuth but not primary."
        },
        {
          "question": "Which tasks are supported by the analyze-text endpoint? (Select 4)",
          "options": [
            "Language Detection",
            "Key Phrase Extraction",
            "Sentiment Analysis",
            "Machine Translation"
          ],
          "answer": [
            "Language Detection",
            "Key Phrase Extraction",
            "Sentiment Analysis",
            "Entity Linking"
          ],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ The REST `analyze-text` supports detection, extraction, sentiment, entities, and linking :contentReference[oaicite:4]{index=4}. Machine Translation is separate."
        },
        {
          "question": "Scenario: You need to find the main topics in user reviews. Which task do you choose?",
          "options": [
            "Entity Linking",
            "Key Phrase Extraction",
            "Sentiment Analysis",
            "Language Detection"
          ],
          "answer": "Key Phrase Extraction",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Key phrases identify the main ideas; entity linking connects entities; sentiment handles mood."
        },
        {
          "question": "What sentiment labels are returned by the service?",
          "options": [
            "Positive/Negative/Neutral/Mixed",
            "Happy/Sad",
            "Agree/Disagree",
            "True/False"
          ],
          "answer": "Positive/Negative/Neutral/Mixed",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Comprehensive sentiment includes Mixed along with positive, neutral, negative :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Which of these is NOT output from detect-language?",
          "options": [
            "Detected language code",
            "Confidence score",
            "Translated text",
            "Document ID"
          ],
          "answer": "Translated text",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Language detection returns ID, code, confidence; does not perform translation."
        },
        {
          "question": "Scenario: Analyze text in French. What property should you set?",
          "options": [
            "analysisInput.language",
            "analysisInput.countryHint",
            "parameters.modelVersion",
            "headers.AcceptLanguage"
          ],
          "answer": "analysisInput.countryHint",
          "type": "hard",
          "explanation": "✅ CountryHint helps language detection when text is such; language property isn’t available at input."
        },
        {
          "question": "What does entity linking produce?",
          "options": [
            "Phrases only",
            "Entities + Wikipedia URLs",
            "Sentiment scores",
            "Language codes"
          ],
          "answer": "Entities + Wikipedia URLs",
          "type": "medium",
          "explanation": "✅ Linking returns candidate entities with reference links such as Wikipedia sources."
        },
        {
          "question": "Which SDK would you install for Python text analysis?",
          "options": [
            "azure-ai-textanalytics",
            "azure-language-sdk",
            "azure-cognitiveservices-nlp",
            "azure-ai-language-analysis"
          ],
          "answer": "azure-ai-textanalytics",
          "type": "medium",
          "explanation": "✅ That is the official Python SDK; others don’t exist or are outdated."
        },
        {
          "question": "Module assessment: True or False — You can perform multiple tasks (like sentiment and key phrases) in one call.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ `analyze-text` supports batching multiple analysis tasks in a single request :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "What confidence range does sentiment API return?",
          "options": ["0.0 to 1.0", "-1 to +1", "1 to 5", "0 to 100"],
          "answer": "0.0 to 1.0",
          "type": "medium",
          "explanation": "✅ Confidence values are normalized from 0 to 1 :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "How many characters per document are supported in standard plan?",
          "options": ["5,120", "10,000", "1,024", "2,147,483,647"],
          "answer": "5,120",
          "type": "hard",
          "explanation": "✅ The limit per document is 5,120 characters :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "Scenario: You only want opinion sentiment about 'battery'. What should you use?",
          "options": [
            "Opinion mining",
            "Key phrase extraction",
            "Entity recognition",
            "Language detection"
          ],
          "answer": "Opinion mining",
          "type": "medium",
          "explanation": "✅ Opinion mining extracts sentiment associated with specific aspects like 'battery' :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which property groups sentence opinions under targets?",
          "options": [
            "sentenceOpinions",
            "sentimentTargets",
            "keyPhraseSentiments",
            "aspectSentiments"
          ],
          "answer": "sentenceOpinions",
          "type": "hard",
          "explanation": "✅ Opinion mining structures results under sentenceOpinions array :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "Which task type finds PII entities like SSNs?",
          "options": [
            "Entity Recognition",
            "Sentiment Analysis",
            "Linked Entities",
            "Key Phrase Extraction"
          ],
          "answer": "Entity Recognition",
          "type": "medium",
          "explanation": "✅ Entity Recognition includes hidden PII types when specified; linking connects entities to Wikipedia."
        },
        {
          "question": "Coding: In C# SDK, which method runs AnalyzeText?",
          "options": [
            "client.AnalyzeTextAsync(...)",
            "client.RunTextTasks(...)",
            "client.TextAnalysis(...)",
            "client.ExecuteAnalysis(...)"
          ],
          "answer": "client.AnalyzeTextAsync(...)",
          "type": "hard",
          "explanation": "✅ That is the pattern used in SDK samples :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "Why use Language Studio over REST?",
          "options": [
            "No code UI to test tasks",
            "REST unsupported tasks",
            "Cheaper",
            "Private network only"
          ],
          "answer": "No code UI to test tasks",
          "type": "medium",
          "explanation": "✅ Language Studio provides UI experience without writing code; pricing and network aren't distinguishing factors."
        },
        {
          "question": "Which output includes a confidence score per sentence?",
          "options": [
            "Sentiment Analysis",
            "Key Phrase Extraction",
            "Entity Linking",
            "Language Detection"
          ],
          "answer": "Sentiment Analysis",
          "type": "medium",
          "explanation": "✅ Sentiment scores are returned per sentence and document."
        },
        {
          "question": "Scenario: Calling REST API you get 429. What happened?",
          "options": [
            "Too many requests throttled",
            "Bad authentication header",
            "Endpoint wrong region",
            "Document too large"
          ],
          "answer": "Too many requests throttled",
          "type": "hard",
          "explanation": "✅ HTTP 429 indicates rate limiting; others are 401, 404, or 413 respectively."
        },
        {
          "question": "What does linked-entity provide that entity-recognition doesn’t?",
          "options": [
            "URL, data source",
            "Confidence score",
            "Text span positions",
            "All entity types"
          ],
          "answer": "URL, data source",
          "type": "medium",
          "explanation": "✅ Linked entities include knowledge base links, entity types and matches; recognition gives type and text only."
        },
        {
          "question": "Module assessment: True or False — Language detection returns a single language only.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ The service returns a single dominant language per document."
        },
        {
          "question": "Which REST parameter sets model version?",
          "options": [
            "parameters.modelVersion",
            "analysisInput.version",
            "headers.X-Model-Version",
            "querystring 'version'"
          ],
          "answer": "parameters.modelVersion",
          "type": "hard",
          "explanation": "✅ In REST body you specify \\\"parameters\\\": {'modelVersion':'latest'} :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "What metadata appears in response if showStats=true?",
          "options": [
            "Character count",
            "Execution region",
            "Billing cost",
            "Model architecture"
          ],
          "answer": "Character count",
          "type": "medium",
          "explanation": "✅ DocumentStatistics with character/token counts are returned when stats enabled :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "Which SDK supports opinion mining explicitly?",
          "options": [
            "azure-ai-textanalytics",
            "azure-cognitiveservices-nlp",
            "azure-language-opinion",
            "azure-ai-language-v3"
          ],
          "answer": "azure-ai-textanalytics",
          "type": "hard",
          "explanation": "✅ The main SDK supports opinion mining; others not supported or deprecated."
        },
        {
          "question": "Which limitation applies to batch size?",
          "options": [
            "Max 1,000 documents",
            "No limit",
            "Max 10 documents per call",
            "Max 100 KB total text"
          ],
          "answer": "Max 1,000 documents",
          "type": "hard",
          "explanation": "✅ The service supports up to 1,000 documents per batch."
        }
      ]
    },
    {
      "name": "Unit 2: Create question answering solutions with Azure AI Language",
      "questions": [
        {
          "question": "What is the primary goal of the question answering capability in Azure AI Language?",
          "options": [
            "Translate documents",
            "Build conversational Q&A over your data",
            "Generate embeddings",
            "Host LLMs"
          ],
          "answer": "Build conversational Q&A over your data",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Question answering adds a conversational layer over data for Q&A applications :contentReference[oaicite:1]{index=1}. Others are unrelated."
        },
        {
          "question": "What is the difference between QnA Maker and custom question answering?",
          "options": [
            "No difference",
            "Custom QnA uses a deep learning ranker and multi-turn support",
            "QnA Maker supports active learning, custom QnA doesn’t",
            "Custom QnA is retired"
          ],
          "answer": "Custom QnA uses a deep learning ranker and multi-turn support",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Custom QnA adds enhanced ranking, multi-turn, region support :contentReference[oaicite:2]{index=2}. Options 1 and 4 are false; 3 is backwards."
        },
        {
          "question": "Which step is required before creating a knowledge base?",
          "options": [
            "Connect to Azure Search",
            "Create a VM",
            "Import data to Cosmos DB",
            "Enable LLM"
          ],
          "answer": "Connect to Azure Search",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Custom QnA relies on Azure Search index for knowledge base search :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "What document source types are supported for QnA projects? (Select 2)",
          "options": [
            "PDF/URL",
            "Word documents",
            "Video files",
            "Azure SQL DB"
          ],
          "answer": ["PDF/URL", "Word documents"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ Supports unstructured docs such as PDF, URL, DOCX. Others aren’t supported :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "What is multi‑turn conversation?",
          "options": [
            "Translation chaining",
            "Q&A with follow-up prompts",
            "Embedding sequence",
            "Single-turn generation"
          ],
          "answer": "Q&A with follow-up prompts",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Multi-turn supports conversational refinement via follow-up prompts :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Scenario: User asks 'My account', what enables guided follow-ups?",
          "options": [
            "Context-free search",
            "Multi-turn prompts pulled from the KB",
            "Active Learning",
            "Language detection"
          ],
          "answer": "Multi-turn prompts pulled from the KB",
          "type": "medium",
          "explanation": "✅ Follow-up prompts in KB guide the next steps :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "How do you add follow-up prompts in Language Studio?",
          "options": [
            "Use \"Answer and prompts\" section",
            "Add custom REST calls",
            "Run CLI script",
            "Automatically enabled"
          ],
          "answer": "Use \"Answer and prompts\" section",
          "type": "hard",
          "explanation": "✅ Studio UI provides Answer & Prompts editor :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "Which REST response property contains follow-up widgets?",
          "options": [
            "answers[0].context.prompts",
            "answers[0].followUps",
            "answers[0].suggestions",
            "answers[0].alternateQuestions"
          ],
          "answer": "answers[0].context.prompts",
          "type": "hard",
          "difficulty": "hard",
          "explanation": "✅ Prompts array appears under `context.prompts` :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "What is active learning in Q&A?",
          "options": [
            "Model training in Azure ML",
            "Suggesting alternate user questions based on real queries",
            "Monitoring request latencies",
            "Indexing embeddings"
          ],
          "answer": "Suggesting alternate user questions based on real queries",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Active learning provides suggestions to improve knowledge base :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which triggers active learning?",
          "options": [
            "Strong ranking delta between top answers",
            "Timer schedule only",
            "All new questions",
            "Vocabulary change"
          ],
          "answer": "Strong ranking delta between top answers",
          "type": "hard",
          "explanation": "✅ When top answers have close scores, suggestions generate :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "How often are active learning suggestions generated?",
          "options": [
            "Every 5 minutes",
            "Every 30 minutes when 5 similar queries cluster",
            "Daily",
            "Never"
          ],
          "answer": "Every 30 minutes when 5 similar queries cluster",
          "type": "hard",
          "explanation": "✅ According to clustering behavior in QnA Maker documentation :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "After suggestions, what must you do to apply changes?",
          "options": [
            "Accept/reject suggestions, Save and Train, Publish",
            "Auto-approved",
            "Refresh index only",
            "Run CLI"
          ],
          "answer": "Accept/reject suggestions, Save and Train, Publish",
          "type": "medium",
          "explanation": "✅ Workflow requires approval, training, publishing :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "Coding: In C# using Azure.AI.Language.QuestionAnswering, which method gets multi-turn answers?",
          "options": [
            "GetAnswersAsync",
            "AskQuestionAsync",
            "GetAnswersWithContextAsync",
            "CreateKnowledgeBaseAsync"
          ],
          "answer": "GetAnswersWithContextAsync",
          "type": "hard",
          "explanation": "✅ That method returns context for multi-turn Q&A."
        },
        {
          "question": "Which client UIs support multi-turn conversation testing?",
          "options": [
            "Language Studio Test Pane",
            "Azure Portal Overview",
            "CLI only",
            "Power BI"
          ],
          "answer": "Language Studio Test Pane",
          "type": "medium",
          "explanation": "✅ Studio provides interactive testing :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "What distinguishes active versus implicit learning?",
          "options": [
            "User selects between close answers vs algorithm clusters scores",
            "Active is manual, implicit is automated login",
            "Both are the same",
            "None"
          ],
          "answer": "User selects between close answers vs algorithm clusters scores",
          "type": "medium",
          "explanation": "✅ Active uses explicit selection; implicit uses internal score differences :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "Which feature adds conversational personality to the KB?",
          "options": [
            "Chit‑chat persona",
            "Custom avatars",
            "Sentiment tuning",
            "Key phrase extraction"
          ],
          "answer": "Chit‑chat persona",
          "type": "medium",
          "explanation": "✅ Chit‑chat adds conversational small talk :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Scenario: Bot responds too formally. How to add warmth?",
          "options": [
            "Add chit‑chat persona",
            "Increase temperature",
            "Add sentiment tuning",
            "Enable active learning"
          ],
          "answer": "Add chit‑chat persona",
          "type": "medium",
          "explanation": "✅ Personas provide tone/styles; others irrelevant :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "Module assess: True or False — You need to publish the KB to enable multi-turn in clients.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ KB must be published so clients can consume its endpoints."
        },
        {
          "question": "What security boundary ensures your Q&A project is protected?",
          "options": [
            "Azure resource + Azure Search & managed identities",
            "Public GitHub instance",
            "Azure Functions only",
            "No security needed"
          ],
          "answer": "Azure resource + Azure Search & managed identities",
          "type": "hard",
          "explanation": "✅ Project is secured via resource integration and identity."
        },
        {
          "question": "Which REST query parameter sets the user ID for active learning?",
          "options": ["userId", "sessionId", "clientUser", "profileId"],
          "answer": "userId",
          "type": "hard",
          "explanation": "✅ API requires userId parameter for session context :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "Scenario: You need support for German and English Q&A. How configure?",
          "options": [
            "Create multi‑language projects during creation",
            "Use runtime parameter to switch",
            "Add language detection first",
            "Use translation API"
          ],
          "answer": "Create multi‑language projects during creation",
          "type": "medium",
          "explanation": "✅ Multi-language projects set at project creation :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "What does layered ranking refer to?",
          "options": [
            "Search index + NLP ranker",
            "GPU + CPU ranking",
            "Language detection + sentiment",
            "Active learning + multi-turn"
          ],
          "answer": "Search index + NLP ranker",
          "type": "medium",
          "explanation": "✅ First search layer, then NLP ranker orders results :contentReference[oaicite:19]{index=19}."
        },
        {
          "question": "Coding: Which SDK package do you install in Python to use custom Q&A?",
          "options": [
            "azure-ai-language-questionanswering",
            "azure-ai-textanalytics",
            "azure-cognitiveservices-languageqa",
            "azure-ai-qnamaker"
          ],
          "answer": "azure-ai-language-questionanswering",
          "type": "hard",
          "explanation": "✅ That’s the official Python package for custom question answering."
        },
        {
          "question": "What is a ‘project’ in custom Q&A?",
          "options": [
            "Encapsulates KB, sources, multi‑turn, active learning",
            "VM for hosting the service",
            "Search index only",
            "LLM deployment"
          ],
          "answer": "Encapsulates KB, sources, multi‑turn, active learning",
          "type": "medium",
          "explanation": "✅ Project bundles all features from source to consumable endpoint :contentReference[oaicite:20]{index=20}."
        },
        {
          "question": "Which action is NOT covered by active learning?",
          "options": [
            "Auto-adding Q&A pairs",
            "Suggesting alternate questions",
            "Training ranker with accepted suggestions",
            "Halting multi-turn context"
          ],
          "answer": "Halting multi-turn context",
          "type": "hard",
          "explanation": "✅ Active learning improves KB, doesn’t impact chat flow."
        }
      ]
    },
    {
      "name": "Unit 3: Create a custom text classification solution",
      "questions": [
        {
          "question": "What defines custom text classification in Azure AI Language?",
          "options": [
            "General sentiment service",
            "Ability to build a model for user-defined categories",
            "Image classification tool",
            "Storage service"
          ],
          "answer": "Ability to build a model for user-defined categories",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Custom text classification lets you define your own categories and build a model. Others are unrelated :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "Which two project types are supported?",
          "options": [
            "Single label classification",
            "Multi label classification",
            "Sentiment analysis only",
            "Translation"
          ],
          "answer": [
            "Single label classification",
            "Multi label classification"
          ],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ The service supports both single- and multi-label projects. Others not supported :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "What is the first step in project lifecycle?",
          "options": [
            "Train model",
            "Define schema and classes",
            "Upload to Cosmos DB",
            "Create VM"
          ],
          "answer": "Define schema and classes",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ After provisioning, first define your classification schema. Training comes later :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "Scenario: You need to classify research papers into domains with only one class per doc. Choose project type:",
          "options": [
            "Multi-label classification",
            "Single-label classification",
            "Sentiment analysis",
            "Group chat"
          ],
          "answer": "Single-label classification",
          "type": "medium",
          "explanation": "✅ One document, one label➡single-label classification."
        },
        {
          "question": "What storage service is required for data ingestion?",
          "options": [
            "Azure Blob Storage",
            "Cosmos DB",
            "File share",
            "Key Vault"
          ],
          "answer": "Azure Blob Storage",
          "type": "medium",
          "explanation": "✅ Custom classification uses Azure Blob to upload .txt or JSON files :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Which permission must be granted on storage account?",
          "options": [
            "Owner",
            "Storage Blob Data Contributor",
            "Reader",
            "DNS Zone Contributor"
          ],
          "answer": "Storage Blob Data Contributor",
          "type": "hard",
          "explanation": "✅ Language resource needs permission to read labeled data blobs :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "How many training jobs can run simultaneously?",
          "options": ["Only one", "Unlimited", "Two", "Ten"],
          "answer": "Only one",
          "type": "medium",
          "explanation": "✅ Only one training job per project can run at a time :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "What split of training/testing is recommended?",
          "options": ["80%-20%", "50%-50%", "90%-10%", "100%-0%"],
          "answer": "80%-20%",
          "type": "medium",
          "explanation": "✅ Recommended split is 80% for training and 20% for testing :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "Scenario: You have a multilingual dataset in English and French. What setting enables both?",
          "options": [
            "Enable multilingual option",
            "Create two projects",
            "Translate all to one language",
            "Use sentiment project"
          ],
          "answer": "Enable multilingual option",
          "type": "medium",
          "explanation": "✅ Multi-lingual option allows multiple languages in same project :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which languages are supported for multilingual?",
          "options": [
            "French, German, Mandarin, Japanese, Korean, etc.",
            "Only English",
            "All languages",
            "Only Latin scripts"
          ],
          "answer": "French, German, Mandarin, Japanese, Korean, etc.",
          "type": "hard",
          "explanation": "✅ Supported include popular languages beyond English :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "Coding: Which REST endpoint imports project data?",
          "options": [
            "POST /language/authoring/analyze-text/projects/{name}/:import",
            "GET /projects/{name}/import",
            "POST /projects/:upload",
            "PATCH /import"
          ],
          "answer": "POST /language/authoring/analyze-text/projects/{name}/:import",
          "type": "hard",
          "explanation": "✅ Import endpoint for labeling data via REST :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "How do you monitor training status via API?",
          "options": [
            "GET using operation-location URL",
            "GET /status only",
            "POST status query",
            "Check portal only"
          ],
          "answer": "GET using operation-location URL",
          "type": "hard",
          "explanation": "✅ You poll the job URL from operation-location header :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "Module assessment: True or False — Training jobs expire after seven days.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Jobs expire after 7 days; but models stay available :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "Which API classifies live texts?",
          "options": [
            "POST /language/analyze-text/jobs",
            "POST /analyze",
            "GET /classify",
            "PUT /model/predict"
          ],
          "answer": "POST /language/analyze-text/jobs",
          "type": "medium",
          "explanation": "✅ Endpoint for runtime classify requests :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "What indicates multi-label output in runtime response?",
          "options": [
            "Multiple category entries ordered by confidence",
            "Single category",
            "Sentiment score",
            "Key phrases"
          ],
          "answer": "Multiple category entries ordered by confidence",
          "type": "medium",
          "explanation": "✅ Multi-label returns array of categories with confidence scores."
        },
        {
          "question": "Scenario: A document belongs to two classes—movie categories. Which classification type?",
          "options": [
            "Multi-label classification",
            "Single-label classification",
            "Sentiment analysis",
            "NER only"
          ],
          "answer": "Multi-label classification",
          "type": "medium",
          "explanation": "✅ Allows assigning multiple labels per document."
        },
        {
          "question": "What performance metrics are shown after training?",
          "options": [
            "Accuracy, precision, recall, F1 score",
            "Latency only",
            "Cost per doc",
            "No metrics"
          ],
          "answer": "Accuracy, precision, recall, F1 score",
          "type": "hard",
          "explanation": "✅ Standard classification metrics are provided on evaluation :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Why label data carefully in single-label project?",
          "options": [
            "Ambiguity lowers accuracy when a document could belong to multiple classes",
            "It doesn’t matter",
            "Model handles ambiguity automatically",
            "System rejects ambiguous documents"
          ],
          "answer": "Ambiguity lowers accuracy when a document could belong to multiple classes",
          "type": "medium",
          "explanation": "✅ Avoid multi-label samples in single-label to prevent mis-train :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "Coding: Which SDK would you use in Python to classify text at runtime?",
          "options": [
            "azure-ai-language",
            "azure-ai-textanalytics",
            "azure-language-runtime",
            "azure-ai-language-text"
          ],
          "answer": "azure-ai-language",
          "type": "hard",
          "explanation": "✅ New unified `azure-ai-language` SDK covers runtime classification."
        },
        {
          "question": "Module assessment: True or False — You can delete projects via Language Studio.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Language Studio allows deleting a project :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "Which of these is NOT part of custom classification lifecycle?",
          "options": [
            "Train model",
            "Deploy to Azure Functions",
            "Label data",
            "Evaluate model"
          ],
          "answer": "Deploy to Azure Functions",
          "type": "medium",
          "explanation": "✅ Deployment refers to model endpoint, not Functions."
        },
        {
          "question": "What enables cross-language classification?",
          "options": [
            "Multilingual model training",
            "Translate before classify",
            "Train separate projects per language",
            "Use sentiment analysis"
          ],
          "answer": "Multilingual model training",
          "type": "hard",
          "explanation": "✅ Multi-lingual option enables direct cross-language predictions :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "Scenario: You need to classify incoming tweets in various languages. Best approach?",
          "options": [
            "Enable multilingual project",
            "Create separate projects",
            "Use detect-language then route",
            "Use sentiment API"
          ],
          "answer": "Enable multilingual project",
          "type": "hard",
          "explanation": "✅ Multilingual project supports all languages in one model."
        },
        {
          "question": "How many docs are in quickstart sample?",
          "options": [
            " ~200 for multi-label, ~210 for single-label",
            "~1000",
            "~10",
            "None"
          ],
          "answer": "~200 for multi-label, ~210 for single-label",
          "type": "hard",
          "explanation": "✅ Quickstart documents counts reported :contentReference[oaicite:19]{index=19}."
        },
        {
          "question": "Why use REST API instead of Studio?",
          "options": [
            "Automate project management",
            "Studio is deprecated",
            "REST is free",
            "Studio can't label"
          ],
          "answer": "Automate project management",
          "type": "medium",
          "explanation": "✅ REST helps script creation/import/training :contentReference[oaicite:20]{index=20}."
        },
        {
          "question": "Which feature allows rebuilding the model with improved data?",
          "options": [
            "Retrain model after label improvements",
            "Train only once",
            "Train on deployment",
            "Label outside studio"
          ],
          "answer": "Retrain model after label improvements",
          "type": "medium",
          "explanation": "✅ You iterate by relabeling and retraining for performance."
        }
      ]
    },
    {
      "name": "Unit 6: Custom Named Entity Recognition",
      "questions": [
        {
          "question": "What defines a custom NER solution in Azure AI Language?",
          "options": [
            "A model for generic entity types",
            "A model trained to extract user-defined entities",
            "An image recognition model",
            "A chatbot"
          ],
          "answer": "A model trained to extract user-defined entities",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Custom NER extracts domain-specific entities you define :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "Which file format is required to import labeled data?",
          "options": [
            "CSV",
            "JSON following project schema",
            "TXT only",
            "Excel"
          ],
          "answer": "JSON following project schema",
          "type": "single",
          "difficulty": "hard",
          "explanation": "✅ Imports require JSON formatted per schema :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "What are the main steps in the custom NER lifecycle? (Select 3)",
          "options": [
            "Define schema",
            "Train model",
            "Label data",
            "Translate data"
          ],
          "answer": ["Define schema", "Label data", "Train model"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ Core steps are schema, labeling, training, then evaluation :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "Scenario: You notice your model misses many addresses. What should you do?",
          "options": [
            "Add more address-labeled examples",
            "Change pricing tier",
            "Enable sentiment analysis",
            "Switch to prebuilt NER"
          ],
          "answer": "Add more address-labeled examples",
          "type": "medium",
          "explanation": "✅ More labeled examples improve recall for that entity."
        },
        {
          "question": "Which metric indicates how many predicted entities are correct?",
          "options": ["Precision", "Recall", "Accuracy", "Latency"],
          "answer": "Precision",
          "type": "medium",
          "explanation": "✅ Precision = TP/(TP+FP) :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Which metric measures ability to find all true entities?",
          "options": ["Recall", "Precision", "F1 Score", "Throughput"],
          "answer": "Recall",
          "type": "medium",
          "explanation": "✅ Recall = TP/(TP+FN) :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "What is the F1 score used for?",
          "options": [
            "Balance between precision and recall",
            "Compute cost",
            "Latency measure",
            "Feature extraction"
          ],
          "answer": "Balance between precision and recall",
          "type": "medium",
          "explanation": "✅ F1 harmonizes precision and recall :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "How long after training does the job record persist?",
          "options": ["7 days", "30 days", "Indefinitely", "24 hours"],
          "answer": "7 days",
          "type": "hard",
          "explanation": "✅ Training job details expire after 7 days :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "True or False: You can run multiple training jobs simultaneously.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ Only one training job per project is allowed :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which permission is needed to connect storage account?",
          "options": [
            "Storage Blob Data Contributor",
            "Reader",
            "VM Contributor",
            "DNS Admin"
          ],
          "answer": "Storage Blob Data Contributor",
          "type": "hard",
          "explanation": "✅ Required for resource to access labeled blobs :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "Which languages are supported for multilingual NER? (Select 2)",
          "options": ["French", "Korean", "Latin", "Only English"],
          "answer": ["French", "Korean"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ Many languages supported including French, Korean :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "Scenario: You trained model in English, query in Japanese. It fails. What next?",
          "options": [
            "Enable multilingual and add Japanese samples",
            "Retrain English only",
            "Switch to prebuilt",
            "Change pricing tier"
          ],
          "answer": "Enable multilingual and add Japanese samples",
          "type": "hard",
          "explanation": "✅ Multilingual setting plus examples improves cross-language extraction :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "What is the API limit for characters per document?",
          "options": [
            "125,000 chars",
            "10,000 chars",
            "512 chars",
            "1 million chars"
          ],
          "answer": "125,000 chars",
          "type": "hard",
          "explanation": "✅ Limit per request is 125K chars :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "How many documents per batch are allowed?",
          "options": ["25", "100", "1000", "Unlimited"],
          "answer": "25",
          "type": "hard",
          "explanation": "✅ Up to 25 documents per batch :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "Which action trims document text automatically?",
          "options": ["Chunking", "Translation", "Summarization", "Embedding"],
          "answer": "Chunking",
          "type": "medium",
          "explanation": "✅ You can chunk text to fit within character limits."
        },
        {
          "question": "Coding: Which REST endpoint starts training?",
          "options": [
            "POST /projects/{name}/:train",
            "GET /models/{id}",
            "POST /analyze",
            "DELETE /project/{name}"
          ],
          "answer": "POST /projects/{name}/:train",
          "type": "hard",
          "explanation": "✅ That endpoint initiates training jobs :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Which JSON field defines entity span?",
          "options": [
            "regionOffset/regionLength",
            "entitySpan",
            "textSpan",
            "start/end"
          ],
          "answer": "regionOffset/regionLength",
          "type": "hard",
          "explanation": "✅ Schema uses regionOffset and regionLength :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "True or False: Empty documents are allowed in storage.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ Documents must contain text; empty files are rejected :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "What does azure-ai-language NER client method return?",
          "options": [
            "Extracted entities with offsets and scores",
            "Only entity text",
            "Only counts",
            "XML response"
          ],
          "answer": "Extracted entities with offsets and scores",
          "type": "medium",
          "explanation": "✅ Runtime gives detailed entity info including position/score."
        },
        {
          "question": "Module assessment: True or False — You can delete a custom NER project in Studio.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Language Studio supports project deletion :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "Why examine the confusion matrix?",
          "options": [
            "To identify entity-type misclassifications",
            "For memory management",
            "To reduce latency",
            "To monitor billing"
          ],
          "answer": "To identify entity-type misclassifications",
          "type": "medium",
          "explanation": "✅ It shows FP/FN cross-entity confusion :contentReference[oaicite:19]{index=19}."
        },
        {
          "question": "Scenario: One label dominates counts in training set. Issue?",
          "options": [
            "Bias toward that label; add more underrepresented samples",
            "Faster training",
            "Nothing",
            "System rejects model"
          ],
          "answer": "Bias toward that label; add more underrepresented samples",
          "type": "hard",
          "explanation": "✅ Imbalance leads to bias; add data for fairness :contentReference[oaicite:20]{index=20}."
        },
        {
          "question": "Which pricing tier allows unlimited projects?",
          "options": ["Standard (S)", "Free (F0)", "Basic", "Premium"],
          "answer": "Standard (S)",
          "type": "medium",
          "explanation": "✅ S tier supports unlimited projects; F0 restricted :contentReference[oaicite:21]{index=21}."
        },
        {
          "question": "How many projects are allowed per resource?",
          "options": ["500", "100", "10", "Unlimited"],
          "answer": "500",
          "type": "hard",
          "explanation": "✅ Up to 500 projects per resource :contentReference[oaicite:22]{index=22}."
        },
        {
          "question": "Which model evaluation data view shows performance per entity type?",
          "options": [
            "Entity type performance tab",
            "Summary only",
            "Deployment logs",
            "JSON only"
          ],
          "answer": "Entity type performance tab",
          "type": "medium",
          "explanation": "✅ Studio shows per-entity metrics in that tab :contentReference[oaicite:23]{index=23}."
        },
        {
          "question": "Why enable managed identity on Language resource?",
          "options": [
            "To authorize storage read access",
            "To improve latency",
            "To reduce cost",
            "To enable translation"
          ],
          "answer": "To authorize storage read access",
          "type": "hard",
          "explanation": "✅ Managed identity grants blob read permissions :contentReference[oaicite:24]{index=24}."
        }
      ]
    },
    {
      "name": "Unit 4: Build a conversational language understanding model",
      "questions": [
        {
          "question": "What is Conversational Language Understanding (CLU)?",
          "options": [
            "A speech-to-text service",
            "A model for intent and entity extraction from text",
            "A translation service",
            "An image classification model"
          ],
          "answer": "A model for intent and entity extraction from text",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ CLU enables extraction of intents and entities from natural language :contentReference[oaicite:1]{index=1}. The others are unrelated."
        },
        {
          "question": "Which of the following can you define in a CLU project? (Select 3)",
          "options": ["Intents", "Entities", "Utterances", "Images"],
          "answer": ["Intents", "Entities", "Utterances"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ You define intents, entities, and utterances. CLU doesn't process images :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "What's a prebuilt entity in CLU?",
          "options": [
            "Quantity.Number",
            "Custom label",
            "Image URL",
            "Audio snippet"
          ],
          "answer": "Quantity.Number",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Quantity.Number is a supported prebuilt entity for numerical extraction :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "Which language SDKs are supported for CLU authoring?",
          "options": ["C# and Python", "JavaScript only", "Java only", "Go"],
          "answer": "C# and Python",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ CLU SDK supports both C# and Python :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "When should you use patterns in CLU?",
          "options": [
            "To differentiate similar utterances with entities",
            "To translate text",
            "For sentiment analysis",
            "To upload images"
          ],
          "answer": "To differentiate similar utterances with entities",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Patterns allow intent differentiation when utterances vary only by entity :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Scenario: Both “Book flight to Paris” and “Book flight to Rome” should map to the same intent. Best approach?",
          "options": [
            "Use a pattern with {City}",
            "Add each as separate intent",
            "Use sentiment analysis",
            "Use translation API"
          ],
          "answer": "Use a pattern with {City}",
          "type": "medium",
          "explanation": "✅ Patterns with placeholder entities handle these variations cleanly."
        },
        {
          "question": "Which of these is NOT a prebuilt entity component?",
          "options": [
            "Quantity.Temperature",
            "Person.Name",
            "Email",
            "Custom AirportCode"
          ],
          "answer": "Custom AirportCode",
          "type": "medium",
          "explanation": "✅ AirportCode isn't prebuilt; others are available :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "What percent split is recommended between training and testing sets?",
          "options": [
            "80% training / 20% testing",
            "100% training / 0% testing",
            "50% / 50%",
            "20% training / 80% testing"
          ],
          "answer": "80% training / 20% testing",
          "type": "medium",
          "explanation": "✅ The module recommends an 80/20 split :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "True or False: Only one training job can run at a time per project.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ CLU limits to one active training job per project :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "Which mode offers multilingual training and better performance?",
          "options": ["Advanced", "Standard", "Basic", "None"],
          "answer": "Advanced",
          "type": "hard",
          "explanation": "✅ Advanced training supports multiple languages with better accuracy :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which REST endpoint initiates CLU training?",
          "options": [
            "POST /language/authoring/analyze-conversations/projects/{project}/:train",
            "GET /train",
            "POST /language/runtime/predict",
            "DELETE /projects/{project}"
          ],
          "answer": "POST /language/authoring/analyze-conversations/projects/{project}/:train",
          "type": "hard",
          "explanation": "✅ This is the documented REST path for training jobs :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "How do you specify pattern intent in payload?",
          "options": [
            "Use sample utterances with entity placeholders",
            "Add JSON field ‘pattern’: true",
            "Use SQL query",
            "Use regex in utterance"
          ],
          "answer": "Use sample utterances with entity placeholders",
          "type": "medium",
          "explanation": "✅ Patterns are defined via utterance examples with placeholders."
        },
        {
          "question": "Which header is required for authoring APIs?",
          "options": [
            "Ocp-Apim-Subscription-Key",
            "Authorization: Bearer",
            "Content-Encoding",
            "Accept-Language"
          ],
          "answer": "Ocp-Apim-Subscription-Key",
          "type": "hard",
          "explanation": "✅ Authoring APIs require subscription key header; bearer is for runtime."
        },
        {
          "question": "Scenario: “Order 5 pizzas” parsed city and number. Which entities should parse '5'?",
          "options": [
            "Quantity.Number",
            "NumberRange",
            "Custom IntEntity",
            "Person.Name"
          ],
          "answer": "Quantity.Number",
          "type": "medium",
          "explanation": "✅ Prebuilt Quantity.Number handles cardinal numbers :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "Module assessment: True or False—CLU supports multilingual prediction without retraining in all languages.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ You need to add some utterances per new language for accuracy :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "Which evaluation metric indicates how often the model predicts correctly?",
          "options": ["Accuracy", "Precision", "Recall", "Latency"],
          "answer": "Accuracy",
          "type": "medium",
          "explanation": "✅ Accuracy is % of correct predictions overall."
        },
        {
          "question": "Coding: Which method fetches predictions using C# runtime SDK?",
          "options": [
            "client.PredictAsync(...)",
            "client.PredictIntentAsync(...)",
            "client.Analyzer.PredictAsync(...)",
            "client.RunPrediction(...)"
          ],
          "answer": "client.PredictIntentAsync(...)",
          "type": "hard",
          "explanation": "✅ C# runtime uses PredictIntentAsync method."
        },
        {
          "question": "In Python, how do you create a new CLU project via CLI?",
          "options": [
            "az language clu project create",
            "az clu create",
            "az language project new",
            "az language clu new-project"
          ],
          "answer": "az language clu project create",
          "type": "hard",
          "explanation": "✅ This is the CLI command for CLU."
        },
        {
          "question": "Why you would use patterns over utterances?",
          "options": [
            "Simplify training when utterance structure is consistent",
            "For sentiment analysis",
            "To translate intent",
            "To avoid training"
          ],
          "answer": "Simplify training when utterance structure is consistent",
          "type": "medium",
          "explanation": "✅ Patterns reduce redundancy when utterance follows template."
        },
        {
          "question": "Which prebuilt entity recognizes email addresses automatically?",
          "options": ["Email", "URL", "Phone Number", "Quantity.Number"],
          "answer": "Email",
          "type": "medium",
          "explanation": "✅ Email is a supported prebuilt component :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "Module assessment: True or False—After publication, your model uses runtime endpoint only, community studio no longer needed.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Published model is consumed via runtime endpoint :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "What does model confidence represent?",
          "options": [
            "Probability the intent/entity is correct",
            "Number of tokens",
            "Latency score",
            "Cost per query"
          ],
          "answer": "Probability the intent/entity is correct",
          "type": "medium",
          "explanation": "✅ Confidence indicates model certainty."
        },
        {
          "question": "Scenario: You see uneven intent performances. What should you do?",
          "options": [
            "Add more labeled utterances for low-performing intents",
            "Retrain delete model",
            "Delete intents",
            "Use sentiment analysis"
          ],
          "answer": "Add more labeled utterances for low-performing intents",
          "type": "medium",
          "explanation": "✅ More training data helps balance intent accuracy."
        },
        {
          "question": "Which metric helps evaluate entity extraction accuracy?",
          "options": [
            "F1 Score",
            "Throughput",
            "Cost per document",
            "Translation rate"
          ],
          "answer": "F1 Score",
          "type": "medium",
          "explanation": "✅ F1 balances precision and recall for entities."
        },
        {
          "question": "True or False: You can edit utterances after publishing model.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ You can update project and retrain as needed."
        },
        {
          "question": "Which builtin prebuilt component handles dates and times?",
          "options": ["Datetime", "Quantity.Number", "Email", "URL"],
          "answer": "Datetime",
          "type": "medium",
          "explanation": "✅ Datetime extracts date/time expressions :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Coding: How do you publish a model via CLI?",
          "options": [
            "az language clu project publish",
            "az clu publish",
            "az language project upload",
            "az language publish-model"
          ],
          "answer": "az language clu project publish",
          "type": "hard",
          "explanation": "✅ Correct CLI command for publishing CLU project."
        }
      ]
    },
    {
      "name": "Unit 5: Custom Named Entity Recognition",
      "questions": [
        {
          "question": "What is Custom Named Entity Recognition (NER)?",
          "options": [
            "Image labeling service",
            "Extracts user-defined entities from unstructured text",
            "Speech-to-text engine",
            "Pretrained translation model"
          ],
          "answer": "Extracts user-defined entities from unstructured text",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Custom NER lets you build models to extract domain-specific entities from text :contentReference[oaicite:1]{index=1}."
        },
        {
          "question": "Which step is performed first in custom NER lifecycle?",
          "options": [
            "Train the model",
            "Define entity schema",
            "Deploy model",
            "Extract entities"
          ],
          "answer": "Define entity schema",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ You must define your entity types before labeling or training :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "Which storage is required for uploading labeled data?",
          "options": ["Azure SQL", "Blob Storage", "File Share", "Cosmos DB"],
          "answer": "Blob Storage",
          "type": "medium",
          "explanation": "✅ Language Studio requires Azure Blob Storage for labeled data files :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "True or False: You can only connect one storage account to a NER resource.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Only a single storage account is allowed per resource :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "Multiple labels allowed on same text span?",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ Entities should not overlap; labeling should be distinct :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Which metric balances Precision and Recall for NER?",
          "options": ["Accuracy", "Latency", "F1 Score", "Throughput"],
          "answer": "F1 Score",
          "type": "medium",
          "explanation": "✅ F1 is the harmonic mean of precision and recall :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "Scenario: Your model misses many entities—what helps?",
          "options": [
            "Add more negative samples",
            "Add more labeled examples for that entity",
            "Switch to translation",
            "Use sentiment analysis"
          ],
          "answer": "Add more labeled examples for that entity",
          "type": "medium",
          "explanation": "✅ More examples help improve recall for that entity."
        },
        {
          "question": "How long do training job records persist?",
          "options": ["24 hours", "7 days", "30 days", "Indefinitely"],
          "answer": "7 days",
          "type": "hard",
          "explanation": "✅ Training job details expire after 7 days :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "True or False: Only one training job can run at a time.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Parallel trainings per project are not allowed :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "What REST endpoint starts training?",
          "options": [
            "POST /trainNER",
            "POST /projects/{proj}/:train",
            "GET /projects/train",
            "POST /ner/train"
          ],
          "answer": "POST /projects/{proj}/:train",
          "type": "hard",
          "explanation": "✅ This is the REST API to initiate training :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which JSON property holds entity span?",
          "options": [
            "startIndex/endIndex",
            "regionOffset/regionLength",
            "entitySpan",
            "spanStart/spanLength"
          ],
          "answer": "regionOffset/regionLength",
          "type": "hard",
          "explanation": "✅ The SDK uses regionOffset and regionLength to define spans :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "Scenario: You have English and French docs—how to support both?",
          "options": [
            "Create separate projects",
            "Enable multilingual option",
            "Translate French manually",
            "Use sentiment mode"
          ],
          "answer": "Enable multilingual option",
          "type": "hard",
          "explanation": "✅ Multilingual mode allows cross-language extraction :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "Which role must you assign to the storage account?",
          "options": [
            "Reader",
            "Blob Data Contributor",
            "Contributor",
            "Virtual Machine User"
          ],
          "answer": "Blob Data Contributor",
          "type": "hard",
          "explanation": "✅ Required for NER service to access data :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "True or False: You can delete a NER project via Language Studio.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Studio supports deletion of projects :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "Which batch size limit applies to runtime analysis?",
          "options": [
            "Max 25 docs",
            "Max 100 docs",
            "Unlimited",
            "Max 10 docs"
          ],
          "answer": "Max 25 docs",
          "type": "hard",
          "explanation": "✅ Runtime NER supports up to 25 documents per batch :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "What's the character limit per document?",
          "options": [
            "50,000 chars",
            "125,000 chars",
            "512 chars",
            "1 million chars"
          ],
          "answer": "125,000 chars",
          "type": "hard",
          "explanation": "✅ API limit per document is 125K characters :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Which performance metric shows proportion of correct extractions?",
          "options": ["Precision", "Recall", "Latency", "Cost"],
          "answer": "Precision",
          "type": "medium",
          "explanation": "✅ Precision measures correct entity extractions among predicted ones."
        },
        {
          "question": "Recall measures:",
          "options": [
            "Correctes predicted/all actual",
            "Latency per doc",
            "Model size",
            "Cost per query"
          ],
          "answer": "Correct predicted/all actual",
          "type": "medium",
          "explanation": "✅ Recall is TP/(TP+FN), capturing missing entities."
        },
        {
          "question": "Why use the confusion matrix?",
          "options": [
            "To see wrong entity types",
            "To evaluate runtime latency",
            "To count docs",
            "To measure cost"
          ],
          "answer": "To see wrong entity types",
          "type": "medium",
          "explanation": "✅ Helps identify which entity types are misclassified :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "Coding: Which Python SDK module handles runtime NER?",
          "options": [
            "azure_ai_language.TextAnalysisClient",
            "azure_ai_language.CustomEntityRecognitionClient",
            "azure_ai_language.KeyPhraseClient",
            "azure_ai_language.EntityLinkingClient"
          ],
          "answer": "azure_ai_language.CustomEntityRecognitionClient",
          "type": "hard",
          "explanation": "✅ This client handles custom NER runtime calls :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "Scenario: Model underperforms—best next step?",
          "options": [
            "Add more labeled data",
            "Delete model",
            "Change region",
            "Use translation API"
          ],
          "answer": "Add more labeled data",
          "type": "medium",
          "explanation": "✅ Additional examples improve model performance."
        },
        {
          "question": "What architecture does Custom NER use?",
          "options": ["Transformer-based", "Rule-based only", "HMM", "SVM"],
          "answer": "Transformer-based",
          "type": "hard",
          "explanation": "✅ Custom NER uses modern transformer models :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "True or False: Custom NER supports active learning.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "❌ Active learning isn't currently offered in custom NER."
        },
        {
          "question": "Which data attribute ensures well-labeled datasets?",
          "options": [
            "Precision labeling guidelines",
            "CSV format only",
            "Machine translation",
            "Random labeling"
          ],
          "answer": "Precision labeling guidelines",
          "type": "medium",
          "explanation": "✅ Precision, consistency, and completeness are crucial :contentReference[oaicite:19]{index=19}."
        },
        {
          "question": "Scenario: You have nested entities—what do you do?",
          "options": [
            "Avoid nesting, split spans",
            "Leave as is",
            "Use multi-label",
            "Switch to text classification"
          ],
          "answer": "Avoid nesting, split spans",
          "type": "hard",
          "explanation": "✅ NER expects non-overlapping spans; you need to separate nested entities :contentReference[oaicite:20]{index=20}."
        },
        {
          "question": "What ensures responsible AI compliance?",
          "options": [
            "Responsible AI notice checked",
            "Model retraining daily",
            "Use only free tier",
            "Use translation API"
          ],
          "answer": "Responsible AI notice checked",
          "type": "medium",
          "explanation": "✅ Users must acknowledge responsible AI when creating projects :contentReference[oaicite:21]{index=21}."
        },
        {
          "question": "How do you delete a custom NER resource?",
          "options": [
            "Via Language Studio delete button",
            "CLI only",
            "Cannot delete",
            "Azure ML portal"
          ],
          "answer": "Via Language Studio delete button",
          "type": "medium",
          "explanation": "✅ Studio UI enables project deletion :contentReference[oaicite:22]{index=22}."
        }
      ]
    },
    {
      "name": "Unit 7: Translate text with Azure AI Translator service",
      "questions": [
        {
          "question": "What is the primary purpose of Azure AI Translator service?",
          "options": [
            "Speech-to-text only",
            "Image translation",
            "Text translation, language detection, transliteration, and dictionaries",
            "Text analytics"
          ],
          "answer": "Text translation, language detection, transliteration, and dictionaries",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ The service supports translation, detection, transliteration, dictionary lookup, etc. :contentReference[oaicite:1]{index=1}."
        },
        {
          "question": "Which operation does Translator support? (Select 3)",
          "options": ["Translate", "Detect", "Transliterate", "Entity Linking"],
          "answer": ["Translate", "Detect", "Transliterate"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ It supports translation, detection, and transliteration; entity linking is a separate service :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "What transliteration does?",
          "options": [
            "Converts text to speech",
            "Translates meaning",
            "Convert script (e.g., Latin script for Chinese)",
            "Detect sentiment"
          ],
          "answer": "Convert script (e.g., Latin script for Chinese)",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Transliteration converts characters to another script :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "Scenario: You need the pinyin for Chinese text. Which parameter do you specify?",
          "options": [
            "toScript=Latn",
            "tone=True",
            "scriptType=pinyin",
            "language=latn"
          ],
          "answer": "toScript=Latn",
          "type": "medium",
          "explanation": "✅ The API supports toScript=Latn for Latin-script transliteration :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "Which endpoint returns supported languages?",
          "options": [
            "GET /languages?api-version=3.0",
            "GET /translate",
            "POST /detect",
            "GET /dictionaries"
          ],
          "answer": "GET /languages?api-version=3.0",
          "type": "hard",
          "explanation": "✅ That endpoint lists supported languages :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Which header is mandatory for translation API calls?",
          "options": [
            "Ocp-Apim-Subscription-Key",
            "Authorization: Bearer",
            "Accept-Encoding",
            "User-Agent"
          ],
          "answer": "Ocp-Apim-Subscription-Key",
          "type": "medium",
          "explanation": "✅ Translator uses subscription key as auth header :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "When using multi-service cognitive resource, which additional header is required?",
          "options": [
            "Ocp-Apim-Subscription-Region",
            "Ocp-Apim-Resource-Id",
            "Accept-Language",
            "X-Region"
          ],
          "answer": "Ocp-Apim-Subscription-Region",
          "type": "hard",
          "explanation": "✅ Region header required for multi-service resources :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "True or False: Language detection can be combined in the same call as translation.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ You can call detect+translate/transliterate in one request :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "Which feature provides alternate translations in context?",
          "options": [
            "Dictionary examples",
            "Sentiment analysis",
            "Entity detection",
            "Key phrase extraction"
          ],
          "answer": "Dictionary examples",
          "type": "medium",
          "explanation": "✅ Dictionary example API returns in-context alternates :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "How many elements max in dictionary examples lookup array?",
          "options": ["10", "25", "50", "100"],
          "answer": "10",
          "type": "hard",
          "explanation": "✅ Dictionary examples limit is 10 elements :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "Which authentication methods exist for Translator?",
          "options": [
            "Subscription key",
            "OAuth2",
            "Managed Identity",
            "All of the above"
          ],
          "answer": "All of the above",
          "type": "medium",
          "explanation": "✅ Translator supports key, OAuth, and managed identity auth options."
        },
        {
          "question": "Scenario: You need domain-specific terminology translations. What feature do you use?",
          "options": [
            "Custom Translator",
            "Sentiment API",
            "Dictionary lookup",
            "Transliteration"
          ],
          "answer": "Custom Translator",
          "type": "medium",
          "explanation": "✅ Custom Translator allows domain-specific translation using custom models :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "What minimum parallel sentences are needed to train a custom model?",
          "options": ["10,000", "1,000", "100,000", "500"],
          "answer": "10,000",
          "type": "hard",
          "explanation": "✅ Custom Translator requires at least 10k parallel sentences :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "What is BLEU score used for?",
          "options": [
            "Evaluate translation quality",
            "Detect language accuracy",
            "Transliterate correctness",
            "Sentiment confidence"
          ],
          "answer": "Evaluate translation quality",
          "type": "medium",
          "explanation": "✅ BLEU measures translation alignment with reference :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "True or False: Custom models can be called via same Translate endpoint specifying category.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Use category param with custom model ID :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "Coding: Which field indicates detected language in response?",
          "options": [
            "detectedLanguage",
            "langDetected",
            "sourceLang",
            "languageDetected"
          ],
          "answer": "detectedLanguage",
          "type": "hard",
          "explanation": "✅ Response JSON uses detectedLanguage object :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Which parameter retrieves Latin script transliteration?",
          "options": ["toScript", "scriptType", "targetScript", "latinTrue"],
          "answer": "toScript",
          "type": "hard",
          "explanation": "✅ API param toScript sets transliteration script :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "Scenario: You want consistent brand translation across docs. You should use:",
          "options": [
            "Glossary",
            "Dictionary lookup only",
            "Language detection",
            "Transliteration"
          ],
          "answer": "Glossary",
          "type": "medium",
          "explanation": "✅ Glossaries enforce consistent terminology :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "Glossaries currently supported for which service?",
          "options": [
            "Document translation",
            "Text translation",
            "Transliteration",
            "Dictionary lookup"
          ],
          "answer": "Document translation",
          "type": "medium",
          "explanation": "✅ Glossary support applies to document translation :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "Module assessment: True or False—Glossary matching is case-sensitive by default.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Glossaries apply only with exact casing :contentReference[oaicite:19]{index=19}."
        },
        {
          "question": "Which endpoint returns dictionary examples?",
          "options": [
            "POST /dictionary/examples",
            "GET /dictionary",
            "POST /translate/dict",
            "GET /examples"
          ],
          "answer": "POST /dictionary/examples",
          "type": "hard",
          "explanation": "✅ That's the documented endpoint :contentReference[oaicite:20]{index=20}."
        },
        {
          "question": "Which error code indicates rate limiting?",
          "options": ["429", "408", "503", "400"],
          "answer": "429",
          "type": "medium",
          "explanation": "✅ HTTP 429 means throttling :contentReference[oaicite:21]{index=21}."
        },
        {
          "question": "Which languages are supported by Translator?",
          "options": [
            "179 languages and variants",
            "50 languages",
            "Only major languages",
            "All languages globally"
          ],
          "answer": "179 languages and variants",
          "type": "hard",
          "explanation": "✅ Translation supports 179 languages/variants :contentReference[oaicite:22]{index=22}."
        },
        {
          "question": "What deployment option allows offline use?",
          "options": [
            "Translator container",
            "CLI tool",
            "Mobile SDK",
            "Edge API"
          ],
          "answer": "Translator container",
          "type": "medium",
          "explanation": "✅ Containers support offline environments :contentReference[oaicite:23]{index=23}."
        },
        {
          "question": "True or False: Dictionary example request body limit is 100 characters per text.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "hard",
          "explanation": "✅ Each element must not exceed 100 chars :contentReference[oaicite:24]{index=24}."
        },
        {
          "question": "Which parameter returns multiple target translations in one call?",
          "options": [
            "to=de&to=fr",
            "multiple=true",
            "languages=de,fr",
            "targetAll=true"
          ],
          "answer": "to=de&to=fr",
          "type": "hard",
          "explanation": "✅ Multiple 'to' query parameters yield multi-language translations :contentReference[oaicite:25]{index=25}."
        }
      ]
    },
    {
      "name": "Unit: Create speech-enabled apps with Azure AI services",
      "questions": [
        {
          "question": "What is the purpose of the Azure AI Speech service?",
          "options": [
            "Process images",
            "Enable speech recognition and synthesis",
            "Run LLMs",
            "Provide chatbot UI"
          ],
          "answer": "Enable speech recognition and synthesis",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ The module teaches how to use speech-to-text and text-to-speech APIs :contentReference[oaicite:1]{index=1}."
        },
        {
          "question": "Which features are supported by the Speech-to-Text API? (Select 3)",
          "options": [
            "Real-time transcription",
            "Batch transcription",
            "Custom speech",
            "Machine translation"
          ],
          "answer": [
            "Real-time transcription",
            "Batch transcription",
            "Custom speech"
          ],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ Supports real-time, batch, and custom speech; translation is a different service :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "Which SDKs support real-time speech-to-text? (Select 2)",
          "options": ["Speech SDK", "Speech CLI", "Vision SDK", "Language SDK"],
          "answer": ["Speech SDK", "Speech CLI"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ Speech SDK and CLI both support real-time transcription :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "True or False: Batch transcription requires audio to be stored in Azure Blob.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Batch transcription processes prerecorded audio from storage :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "What parameter controls output detail level in RecognitionConfig?",
          "options": ["detailed", "outputFormat", "formatLevel", "verbosity"],
          "answer": "outputFormat",
          "type": "hard",
          "explanation": "✅ You can choose between simple and detailed formats :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Which quickstart method transcribes one utterance up to 30 seconds?",
          "options": [
            "RecognizeOnceAsync",
            "StartContinuousRecognition",
            "RecognizeFileAsync",
            "BatchTranscribeAsync"
          ],
          "answer": "RecognizeOnceAsync",
          "type": "medium",
          "explanation": "✅ RecognizeOnceAsync handles short utterances :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "Scenario: You want speaker diarization. Which feature to enable?",
          "options": [
            "speakerDiarization",
            "speakerTagging",
            "speakerSeparation",
            "speakerIdentify"
          ],
          "answer": "speakerDiarization",
          "type": "medium",
          "explanation": "✅ The module mentions 'Speaker diarization' config :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "Which SSML element adjusts voice pitch or rate?",
          "options": ["<prosody>", "<break>", "<voice>", "<audio>"],
          "answer": "<prosody>",
          "type": "hard",
          "explanation": "✅ SSML lets you control pitch, rate, volume via <prosody> :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "True or False: You must include environment variables for SPEECH_KEY and ENDPOINT.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Quickstarts require these env vars to run :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which method is used in .NET to synthesize text to speech?",
          "options": [
            "SpeakTextAsync",
            "SynthesizeAsync",
            "ToSpeechAsync",
            "RenderSpeechAsync"
          ],
          "answer": "SpeakTextAsync",
          "type": "easy",
          "explanation": "✅ Quickstart uses SpeakTextAsync :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "When using SSML, which element allows switching voice mid-document?",
          "options": ["<voice>", "<switch>", "<speaker>", "<segment>"],
          "answer": "<voice>",
          "type": "medium",
          "explanation": "✅ SSML lets you embed multiple <voice> tags :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "Which format(s) can synthesized audio be saved to? (Select 2)",
          "options": ["MP3", "WAV", "PDF", "TXT"],
          "answer": ["MP3", "WAV"],
          "type": "medium",
          "explanation": "✅ Audio formats include MP3 and WAV; PDF/TXT irrelevant :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "Scenario: You want to animate lips. Which event do you use?",
          "options": ["viseme", "bookmark", "break", "audio"],
          "answer": "viseme",
          "type": "hard",
          "explanation": "✅ Viseme events produce mouth shapes :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "Which SSML tag adds a pause between sentences?",
          "options": ["<break>", "<silence>", "<pause>", "<gap>"],
          "answer": "<break>",
          "type": "medium",
          "explanation": "✅ The <break> element defines pauses :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "What determines voice accent for English text?",
          "options": [
            "Voice name",
            "Language code only",
            "Prosody rate",
            "Synth format"
          ],
          "answer": "Voice name",
          "type": "medium",
          "explanation": "✅ Example uses Spanish-named voice to add accent :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "True or False: All neural voices support viseme events.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "hard",
          "explanation": "❌ Only en‑US neural voices support viseme :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "Which role helps secure credentials in production?",
          "options": [
            "Managed Identity",
            "ClientCertificate",
            "SharedKeyLite",
            "Anonymous"
          ],
          "answer": "Managed Identity",
          "type": "medium",
          "explanation": "✅ Guidelines recommend managed identity over key in prod :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "In Python/Speech SDK, which class loads audio from file?",
          "options": [
            "AudioConfig.from_wav_file_input",
            "AudioConfig.from_mic",
            "AudioFileReader",
            "SpeechAudioInput"
          ],
          "answer": "AudioConfig.from_wav_file_input",
          "type": "hard",
          "explanation": "✅ Python quickstart uses this method :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "Scenario: Deployed to App Service and mic fails. What caused it?",
          "options": [
            "Using default microphone instead of file input",
            "Missing ENdpoint env var",
            "Wrong subscription key",
            "Too much latency"
          ],
          "answer": "Using default microphone instead of file input",
          "type": "hard",
          "explanation": "✅ App Service has no mic; use upload and file input :contentReference[oaicite:19]{index=19}."
        },
        {
          "question": "Module: True or False — You can use multiple voices in a single SSML file.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ SSML supports multiple <voice> elements :contentReference[oaicite:20]{index=20}."
        },
        {
          "question": "Which SSML element adds custom audio clips?",
          "options": ["<audio>", "<sound>", "<media>", "<clip>"],
          "answer": "<audio>",
          "type": "hard",
          "explanation": "✅ <audio> inserts prerecorded clips :contentReference[oaicite:21]{index=21}."
        },
        {
          "question": "What is the purpose of SSML?",
          "options": [
            "Fine-tune speech output (pitch, rate, volume)",
            "Connect to microphone",
            "Store text logs",
            "Create image files"
          ],
          "answer": "Fine-tune speech output (pitch, rate, volume)",
          "type": "medium",
          "explanation": "✅ SSML adjusts prosody and delivery :contentReference[oaicite:22]{index=22}."
        },
        {
          "question": "Which REST endpoint synthesizes SSML to speech?",
          "options": [
            "POST /cognitiveservices/v1",
            "GET /speech/synthesize",
            "POST /speak",
            "PUT /synthesis"
          ],
          "answer": "POST /cognitiveservices/v1",
          "type": "hard",
          "explanation": "✅ REST SSML uses cognitiveservices/v1 :contentReference[oaicite:23]{index=23}."
        },
        {
          "question": "Scenario: You need multilingual output with synced accents. Which element config?",
          "options": [
            "<voice name='es-ES-ElviraNeural'>",
            "<voice name='en-GB-RobertNeural'>",
            "<prosody variant='spanish'>",
            "<audio src='spanish.mp3'>"
          ],
          "answer": "<voice name='es-ES-ElviraNeural'>",
          "type": "medium",
          "explanation": "✅ Named voice sets accent and multilingual support :contentReference[oaicite:24]{index=24}."
        },
        {
          "question": "Which element inserts a 2-second silence?",
          "options": [
            "<break time=\"2000ms\"/>",
            "<pause duration=\"2s\"/>",
            "<silence length=\"2\"/>",
            "<break time=\"2\"/>"
          ],
          "answer": "<break time=\"2000ms\"/>",
          "type": "medium",
          "explanation": "✅ SSML <break time> uses millisecond units :contentReference[oaicite:25]{index=25}."
        },
        {
          "question": "How are neural voices billed?",
          "options": [
            "Per character including SSML tags",
            "Per minute",
            "Flat monthly fee",
            "Free"
          ],
          "answer": "Per character including SSML tags",
          "type": "hard",
          "explanation": "✅ Billed per character, including SSML markup :contentReference[oaicite:26]{index=26}."
        },
        {
          "question": "Scenario: You need phoneme-level pronunciation control. Which SSML tag do you use?",
          "options": ["<phoneme>", "<prosody>", "<voice>", "<audio>"],
          "answer": "<phoneme>",
          "type": "hard",
          "explanation": "✅ SSML supports <phoneme> for pronunciation :contentReference[oaicite:27]{index=27}."
        },
        {
          "question": "True or False: You must select voice by name in REST endpoint.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ SSML and REST endpoint require explicit voice name :contentReference[oaicite:28]{index=28}."
        }
      ]
    },
    {
      "name": "Unit 9: Translate Speech with the Azure AI Speech service",
      "questions": [
        {
          "question": "What are the main capabilities of Azure Speech Translation?",
          "options": [
            "Speech to text translation",
            "Speech to speech translation",
            "Speaker recognition",
            "Speech to speech and text translation"
          ],
          "answer": "Speech to text translation",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ The module focuses on speech-to-text translation. Speech-to-speech builds on that but isn't the core here :contentReference[oaicite:1]{index=1}."
        },
        {
          "question": "Which SDK supports real-time speech translation? (Select 2)",
          "options": ["Speech SDK", "Speech CLI", "REST API", "Language SDK"],
          "answer": ["Speech SDK", "Speech CLI"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ Both SDK and CLI enable real-time translation; REST isn't used for live translation :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "True or False: You must specify a source language when using multi-lingual translation.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ Multi‑lingual mode auto-detects languages during translation :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "What config object initializes speech translation in C#?",
          "options": [
            "SpeechRecognitionConfig",
            "SpeechTranslationConfig",
            "TranslateConfig",
            "AudioTranslationConfig"
          ],
          "answer": "SpeechTranslationConfig",
          "type": "medium",
          "explanation": "✅ The module uses SpeechTranslationConfig.FromSubscription(...) :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "Which method performs single-shot speech translation?",
          "options": [
            "RecognizeOnceAsync",
            "TranslateOnceAsync",
            "RecognizeAsync",
            "TranslateSpeechAsync"
          ],
          "answer": "RecognizeOnceAsync",
          "type": "hard",
          "explanation": "✅ Quickstart uses RecognizeOnceAsync for single utterance translation :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Scenario: You need translation of Spanish audio to German text and French speech. Which config?",
          "options": [
            "Add both 'de' and 'fr' as target languages",
            "Set source to 'es-DE'",
            "Use multilingual mode without targets",
            "Use RecognizeOnce without translating"
          ],
          "answer": "Add both 'de' and 'fr' as target languages",
          "type": "medium",
          "explanation": "✅ Multiple target languages supported by specifying each target :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "Which header is required for authorization in translation API?",
          "options": [
            "Ocp-Apim-Subscription-Key",
            "Authorization: Bearer",
            "Api-Key",
            "X-Auth"
          ],
          "answer": "Ocp-Apim-Subscription-Key",
          "type": "medium",
          "explanation": "✅ Speech service uses subscription key header :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "True or False: The Speech CLI can translate microphone input.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ CLI example shows spx translate --microphone :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "What output does RecognizeOnceAsync return on success?",
          "options": [
            "Translated text only",
            "Source language text",
            "Recognition reason",
            "Both recognized and translated text"
          ],
          "answer": "Both recognized and translated text",
          "type": "medium",
          "explanation": "✅ Result has both source and translated texts :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which SDK language is NOT supported for speech translation according to quickstart docs?",
          "options": ["Go", "C#", "Python", "JavaScript"],
          "answer": "Go",
          "type": "hard",
          "explanation": "✅ Go SDK doesn’t support translate speech :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "How do you specify audio file input in Python?",
          "options": [
            "AudioConfig.from_wav_file_input",
            "AudioConfig.from_mic",
            "AudioConfig.file",
            "AudioConfig.load()"
          ],
          "answer": "AudioConfig.from_wav_file_input",
          "type": "medium",
          "explanation": "✅ Quickstart uses this for file-based translation :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "Which feature handles language switching mid-session?",
          "options": [
            "Multi-lingual speech translation",
            "Batch translation",
            "Single-language mode",
            "Speech-to-text"
          ],
          "answer": "Multi-lingual speech translation",
          "type": "medium",
          "explanation": "✅ Supports session with multiple languages without restart :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "Module question: True or False—Extended speech translation supports more than two target languages free of charge.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "hard",
          "explanation": "✅ Third language incurs cost; free tier includes only two :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "Which command sets credentials for CLI?",
          "options": [
            "spx config @key --set",
            "spx login",
            "az speech login",
            "spx set credentials"
          ],
          "answer": "spx config @key --set",
          "type": "hard",
          "explanation": "✅ CLI uses spx config to set key and region :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "Coding: Which Python class handles translation config?",
          "options": [
            "speechsdk.translation.SpeechTranslationConfig",
            "speechsdk.SpeechConfig",
            "speechsdk.TranslationRecognizerConfig",
            "speechsdk.Constants"
          ],
          "answer": "speechsdk.translation.SpeechTranslationConfig",
          "type": "hard",
          "explanation": "✅ Used in quickstart code :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Why use multi-lingual mode over regular translation config?",
          "options": [
            "Supports auto source detection",
            "Improves voice quality",
            "Enables speaker ID",
            "Speeds up translation"
          ],
          "answer": "Supports auto source detection",
          "type": "medium",
          "explanation": "✅ Multi-lingual mode can identify source languages dynamically :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "Scenario: You need to translate a recorded MP4 video. What must you prepare?",
          "options": [
            "Install GStreamer and use PullAudioInputStream",
            "Direct microphone input works",
            "Use REST API",
            "Convert video to text first"
          ],
          "answer": "Install GStreamer and use PullAudioInputStream",
          "type": "hard",
          "explanation": "✅ For compressed MP4, you need GStreamer support points :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "What are ‘interim results’?",
          "options": [
            "Translations delivered before final silence",
            "Only audio samples",
            "Errors during translation",
            "Batch transcription logs"
          ],
          "answer": "Translations delivered before final silence",
          "type": "medium",
          "explanation": "✅ Interim transcription/translation visible during streaming :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "Module assessment: True or False—Speaker recognition is required for translation.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ Speaker recognition is unrelated to translation :contentReference[oaicite:19]{index=19}."
        },
        {
          "question": "Which reason indicates translation success in C# result?",
          "options": [
            "TranslatedSpeech",
            "RecognizedSpeech",
            "Canceled",
            "NoMatch"
          ],
          "answer": "TranslatedSpeech",
          "type": "hard",
          "explanation": "✅ result.Reason == ResultReason.TranslatedSpeech indicates success :contentReference[oaicite:20]{index=20}."
        },
        {
          "question": "True or False: TranslationRecognizer supports continuous recognition.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ It supports continuous translation using start/stop methods :contentReference[oaicite:21]{index=21}."
        },
        {
          "question": "Which is the default timeout for single-shot translation before silence?",
          "options": ["15 seconds", "30 seconds", "60 seconds", "No timeout"],
          "answer": "15 seconds",
          "type": "hard",
          "explanation": "✅ RecognizeOnceAsync times out at ~15s or silence :contentReference[oaicite:22]{index=22}."
        },
        {
          "question": "Coding: How do you close the recognizer in JS?",
          "options": [
            "translationRecognizer.close()",
            "translationRecognizer.stop()",
            "translationRecognizer.dispose()",
            "translationRecognizer.end()"
          ],
          "answer": "translationRecognizer.close()",
          "type": "hard",
          "explanation": "✅ Node.js sample uses close after use :contentReference[oaicite:23]{index=23}."
        },
        {
          "question": "Why set multiple 'target' languages in config?",
          "options": [
            "To receive parallel translations in one call",
            "To improve accuracy",
            "To reduce latency",
            "To increase voice options"
          ],
          "answer": "To receive parallel translations in one call",
          "type": "medium",
          "explanation": "✅ Multiple targets simultaneously supported :contentReference[oaicite:24]{index=24}."
        },
        {
          "question": "Scenario: Recognizer returns RecognizedSpeech but no translations. What’s wrong?",
          "options": [
            "No target languages added",
            "Key is invalid",
            "Audio file corrupt",
            "Region mismatch"
          ],
          "answer": "No target languages added",
          "type": "medium",
          "explanation": "✅ If no targets are set, translation won't occur."
        }
      ]
    },
    {
      "name": "Unit 10: Develop an audio-enabled generative AI application",
      "questions": [
        {
          "question": "What is the main goal of this module?",
          "options": [
            "Deploy audio chat bots",
            "Deploy text-only LLM",
            "Build image classifiers",
            "Create video editors"
          ],
          "answer": "Deploy audio chat bots",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ The module focuses on deploying audio-enabled chat apps via Azure AI Foundry :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "Which service do you deploy to handle audio-enabled generative AI?",
          "options": [
            "Azure AI Foundry multimodal model",
            "Azure OpenAI text-only",
            "Azure Video Indexer",
            "Azure Bot Service"
          ],
          "answer": "Azure AI Foundry multimodal model",
          "type": "medium",
          "explanation": "✅ You deploy a multimodal model (text + audio) in Azure AI Foundry :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "Which model currently supports audio & text inputs?",
          "options": ["Phi-4-multimodal-instruct", "GPT-3.5", "Phi-2", "Codex"],
          "answer": "Phi-4-multimodal-instruct",
          "type": "medium",
          "explanation": "✅ The Phi‑4 multimodal model supports audio plus text :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "True or False: You can send text, audio, and image in a single prompt.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ The model doesn't support all three modalities at once :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Which SDK is used to build the audio chat app in Python?",
          "options": [
            "azure-ai-inference ChatCompletionsClient",
            "azure-language TextAnalyticsClient",
            "azure-cognitiveservices-speech SpeechRecognizer",
            "azure-identity DefaultAzureCredential"
          ],
          "answer": "azure-ai-inference ChatCompletionsClient",
          "type": "medium",
          "explanation": "✅ The module uses ChatCompletionsClient from inference SDK :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "What env var holds the audio file path?",
          "options": [
            "AUDIO_FILE",
            "SPEECH_FILE",
            "INPUT_AUDIO",
            "AUDIO_SOURCE"
          ],
          "answer": "AUDIO_FILE",
          "type": "hard",
          "explanation": "✅ The lab uses AUDIO_FILE for audio in examples."
        },
        {
          "question": "Scenario: You get 'invalid input' error sending MP3 audio. What’s likely wrong?",
          "options": [
            "Audio format unsupported",
            "Model not deployed",
            "Key expired",
            "Wrong endpoint"
          ],
          "answer": "Audio format unsupported",
          "type": "medium",
          "explanation": "✅ The model may reject unsupported formats like MP3 :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "Which ROS parameter sets model name in code?",
          "options": ["model", "deployment_name", "model_id", "name"],
          "answer": "model",
          "type": "hard",
          "explanation": "✅ In client code you specify `model=…`:contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "Which MIME type is used for audio input?",
          "options": [
            "audio/wav",
            "text/plain",
            "image/png",
            "application/json"
          ],
          "answer": "audio/wav",
          "type": "medium",
          "explanation": "✅ WAV is the supported audio format in examples :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which voice model is documented in GPT‑4o audio quickstart?",
          "options": ["Alloy", "Echo", "Shimmer", "All of the above"],
          "answer": "All of the above",
          "type": "medium",
          "explanation": "✅ Alloy, Echo, and Shimmer are supported voices :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "True or False: Audio input must be ≤20 MB for GPT‑4o models.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Max audio file size limit is 20 MB :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "Multiple-choice: Which capabilities are supported by Phi-4o-Audio? (Select 2)",
          "options": [
            "Audio input comprehension",
            "Audio output generation",
            "Video analysis",
            "Image generation"
          ],
          "answer": ["Audio input comprehension", "Audio output generation"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ It supports both audio in and audio out :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "Scenario: Your chat app plays silence. What’s missing?",
          "options": [
            "Audio decoding step",
            "Text prompt",
            "Authentication header",
            "Model deployment"
          ],
          "answer": "Audio decoding step",
          "type": "medium",
          "explanation": "✅ You must decode returned base64 audio to play sound properly."
        },
        {
          "question": "Which Python method sends a chat message?",
          "options": [
            "client.get_chat_completion",
            "client.create_chat",
            "client.send_audio",
            "client.chat()"
          ],
          "answer": "client.get_chat_completion",
          "type": "hard",
          "explanation": "✅ The inference SDK uses get_chat_completion :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "Why avoid combining text, audio, image in single prompt?",
          "options": [
            "Model limitation",
            "Licensing cost",
            "Domain restrictions",
            "Network latency"
          ],
          "answer": "Model limitation",
          "type": "medium",
          "explanation": "✅ The model doesn't allow all three at once :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "Module assess: True or False—Audio output is base64-encoded in response.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Examples show audio returned as base64 in JSON."
        },
        {
          "question": "Which format is used for SSML in audio prompts?",
          "options": ["<speak>", "<audio>", "<voice>", "<model>"],
          "answer": "<speak>",
          "type": "medium",
          "explanation": "✅ Standard SSML root tag <speak> used :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Why set `contentType=audio/*` in chat client?",
          "options": [
            "To indicate audio input",
            "For translation only",
            "To enable subtitles",
            "For text-only mode"
          ],
          "answer": "To indicate audio input",
          "type": "medium",
          "explanation": "✅ You must clarify that your input is audio."
        },
        {
          "question": "True or False: The inference client supports Java SDK for audio multimodal.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "hard",
          "explanation": "✅ Audio multimodal isn’t available in Java SDK yet :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "Scenario: You need to stream audio as it's being generated. Which mode?",
          "options": [
            "Streaming chat completion",
            "Batch chat",
            "Prompt flow only",
            "CLI mode"
          ],
          "answer": "Streaming chat completion",
          "type": "medium",
          "explanation": "✅ Streaming enables partial audio/text output as generated."
        },
        {
          "question": "What MIME type indicates streamed audio in HTTP response?",
          "options": [
            "audio/wav",
            "text/event-stream",
            "application/octet-stream",
            "multipart/mixed"
          ],
          "answer": "text/event-stream",
          "type": "hard",
          "explanation": "✅ Chat completions streaming uses event-stream MIME type."
        },
        {
          "question": "Why include `audio=...` field in request JSON?",
          "options": [
            "To pass audio bytes",
            "To set volume",
            "To specify voice name",
            "To enable emotions"
          ],
          "answer": "To pass audio bytes",
          "type": "medium",
          "explanation": "✅ You embed input audio in JSON for multimodal prompts."
        },
        {
          "question": "Coding: Which Python package must you install?",
          "options": [
            "azure-ai-inference",
            "azure-ai-language",
            "azure-cognitiveservices-speech",
            "azure-ai-textanalytics"
          ],
          "answer": "azure-ai-inference",
          "type": "medium",
          "explanation": "✅ The audio multimodal examples reference azure‑ai‑inference :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "Which scenario suits fallback: audio->text->image?",
          "options": [
            "User sends audio with embedded image description",
            "Text-only chat",
            "Video processing",
            "Translation only"
          ],
          "answer": "User sends audio with embedded image description",
          "type": "hard",
          "explanation": "✅ You might cascade modalities, e.g., first transcribe, then add context."
        },
        {
          "question": "True or False: Model can summarize audio context in text output.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ The model can interpret tone and summarize audio context :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "Which caveat is highlighted for previews?",
          "options": [
            "No SLA",
            "Limited region availability",
            "Feature may change",
            "All of the above"
          ],
          "answer": "All of the above",
          "type": "medium",
          "explanation": "✅ Preview features have no SLA, limited regions, and may change :contentReference[oaicite:19]{index=19}."
        }
      ]
    }
  ]
}
