{
  "units": [
    {
      "name": "Unit 1: Analyze images",
      "questions": [
        {
          "question": "What is the primary purpose of Azure AI Vision Image Analysis?",
          "options": [
            "Process speech",
            "Analyze visual content of images using pre-trained models",
            "Translate text",
            "Host LLMs"
          ],
          "answer": "Analyze visual content of images using pre-trained models",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ The service extracts captions, tags, OCR, and object/location info from images :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "Which feature(s) can be extracted from an image? (Select 3)",
          "options": [
            "Caption",
            "OCR (Read)",
            "Dense captions",
            "Audio transcription"
          ],
          "answer": ["Caption", "OCR (Read)", "Dense captions"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ Supported features include caption, OCR, dense captions; audio transcription is unrelated :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "What image formats are supported for analysis? (Select 2)",
          "options": ["JPEG", "PNG", "MP4", "TXT"],
          "answer": ["JPEG", "PNG"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ Valid formats include JPEG and PNG; MP4 and TXT are invalid :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "What is the maximum image size supported?",
          "options": ["20 MB", "100 MB", "500 KB", "Unlimited"],
          "answer": "20 MB",
          "type": "medium",
          "explanation": "✅ The limit per image is 20 MB :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "What minimum image dimension is required?",
          "options": [
            "50×50 pixels",
            "10×10 pixels",
            "1920×1080",
            "No minimum"
          ],
          "answer": "50×50 pixels",
          "type": "medium",
          "explanation": "✅ Size must be at least 50×50 px :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "Scenario: You need a human-readable caption for an image using code. Which VisualFeature enum is needed?",
          "options": [
            "VisualFeatures.CAPTION",
            "VisualFeatures.FACES",
            "VisualFeatures.COLOR",
            "VisualFeatures.AUDIO"
          ],
          "answer": "VisualFeatures.CAPTION",
          "type": "medium",
          "explanation": "✅ CAPTION extracts image description; others are irrelevant :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "Which authentication methods are supported? (Select 2)",
          "options": [
            "API key",
            "Managed Identity (Entra ID)",
            "OAuth2 token",
            "SSH key"
          ],
          "answer": ["API key", "Managed Identity (Entra ID)"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ Supported auth includes key and Entra ID, not SSH or OAuth2 explicitly :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "True or False: Gender-neutral captions can be enabled.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Code can set `gender_neutral_caption=True` :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which client constructor parameter is required for key authentication in Python?",
          "options": [
            "AzureKeyCredential(key)",
            "DefaultAzureCredential",
            "OAuthToken",
            "SSHKey"
          ],
          "answer": "AzureKeyCredential(key)",
          "type": "medium",
          "explanation": "✅ Sample uses AzureKeyCredential(key) :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "How do you analyze an image from URL in Python?",
          "options": [
            "client.analyze_from_url(...)",
            "client.read_text(...)",
            "client.detect_image_url(...)",
            "client.url_analyze(...)"
          ],
          "answer": "client.analyze_from_url(...)",
          "type": "hard",
          "explanation": "✅ URL analysis uses analyze_from_url(...) :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "True or False: OCR uses async endpoint separate from vision calls.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ OCR is integrated via VisualFeatures.READ in same call :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "Which visual feature detects people and their bounding boxes?",
          "options": [
            "VisualFeatures.PEOPLE",
            "VisualFeatures.OBJECTS",
            "VisualFeatures.TAGS",
            "VisualFeatures.THUMBNAILS"
          ],
          "answer": "VisualFeatures.PEOPLE",
          "type": "medium",
          "explanation": "✅ PEOPLE detects humans, OBJECTS detect objects broadly :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "Scenario: You need to generate smart thumbnails. Which feature is required?",
          "options": [
            "VisualFeatures.SMART_CROPS",
            "VisualFeatures.CAPTION",
            "VisualFeatures.TAGS",
            "VisualFeatures.READ"
          ],
          "answer": "VisualFeatures.SMART_CROPS",
          "type": "hard",
          "explanation": "✅ SMART_CROPS identifies thumbnail-ready regions :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "What exception is raised for a 401 Unauthorized error?",
          "options": [
            "HttpResponseError",
            "KeyError",
            "AuthException",
            "ValueError"
          ],
          "answer": "HttpResponseError",
          "type": "hard",
          "explanation": "✅ SDK raises HttpResponseError on non-success codes :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Which pip package should you install for the Python Image Analysis client?",
          "options": [
            "azure-ai-vision-imageanalysis",
            "azure-cognitiveservices-vision",
            "azure-vision",
            "azure-computer-vision"
          ],
          "answer": "azure-ai-vision-imageanalysis",
          "type": "hard",
          "explanation": "✅ Package name as per docs :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "True or False: You can perform image analysis using environment variables alone without code.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ You need code to create client and call `analyze` :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "Which bounding structure does OCR return for text lines?",
          "options": ["bounding_polygon", "rectangle", "box2D", "bboxList"],
          "answer": "bounding_polygon",
          "type": "hard",
          "explanation": "✅ OCR provides bounding_polygon for each line :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "Scenario: You receive a 'format not supported' error. Likely cause?",
          "options": [
            "Wrong image format",
            "Insufficient permissions",
            "Model deprecated",
            "Service not provisioned"
          ],
          "answer": "Wrong image format",
          "type": "medium",
          "explanation": "✅ Happens if image isn't JPEG/PNG/GIF/BMP/WEBP/ICO/TIFF :contentReference[oaicite:19]{index=19}."
        },
        {
          "question": "Which code block sets up async client in Python?",
          "options": [
            "from azure.ai.vision.imageanalysis.aio import ImageAnalysisClient",
            "import asyncclient",
            "from azure.async import VisionClient",
            "import ImageAnalysisAsync"
          ],
          "answer": "from azure.ai.vision.imageanalysis.aio import ImageAnalysisClient",
          "type": "hard",
          "explanation": "✅ Async client import documented :contentReference[oaicite:20]{index=20}."
        },
        {
          "question": "How do you request multiple features in a single analyze call?",
          "options": [
            "visual_features=[VisualFeatures.CAPTION, VisualFeatures.READ]",
            "options=*",
            "featuresAll=true",
            "multi_feature=yes"
          ],
          "answer": "visual_features=[VisualFeatures.CAPTION, VisualFeatures.READ]",
          "type": "medium",
          "explanation": "✅ showing list usage in samples :contentReference[oaicite:21]{index=21}."
        },
        {
          "question": "Which .NET enum represents tags feature?",
          "options": [
            "VisualFeatureTypes.Tags",
            "VisualFeatures.TAGS",
            "Feature.Tags",
            "ImageTags"
          ],
          "answer": "VisualFeatureTypes.Tags",
          "type": "hard",
          "explanation": "✅ As shown in AI-102 lab code :contentReference[oaicite:22]{index=22}."
        },
        {
          "question": "True or False: Analyze image returns both tags and categories by default.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ You must specify categories in features parameter :contentReference[oaicite:23]{index=23}."
        },
        {
          "question": "Which visual feature detects adult or racy content?",
          "options": [
            "VisualFeatureTypes.Adult",
            "VisualFeatures.SAFETY",
            "VisualFeatures.ADULT",
            "VisualFeatureTypes.Safety"
          ],
          "answer": "VisualFeatureTypes.Adult",
          "type": "medium",
          "explanation": "✅ Adult detection uses VisualFeatureTypes.Adult :contentReference[oaicite:24]{index=24}."
        },
        {
          "question": "Scenario: You want to identify brands/logos — which feature to include?",
          "options": [
            "VisualFeatureTypes.Brands",
            "VisualFeatures.LOGO",
            "VisualFeatures.BRAND",
            "VisualFeatureTypes.Tags"
          ],
          "answer": "VisualFeatureTypes.Brands",
          "type": "medium",
          "explanation": "✅ Used in AI-102 lab :contentReference[oaicite:25]{index=25}."
        },
        {
          "question": "What is the confidence range for captions?",
          "options": ["0 to 1", "1 to 100", "0 to 10", "0 to 1000"],
          "answer": "0 to 1",
          "type": "medium",
          "explanation": "✅ Captions include confidence float between 0 and 1 :contentReference[oaicite:26]{index=26}."
        },
        {
          "question": "Coding: How do you print tag names in C# quickstart?",
          "options": [
            "foreach(var tag in tagsResult.Tags) Console.WriteLine(tag.Name)",
            "tagsResult.print()",
            "Console.Write(tagsResult)",
            "tagsResult.tags.foreach(print)"
          ],
          "answer": "foreach(var tag in tagsResult.Tags) Console.WriteLine(tag.Name)",
          "type": "hard",
          "explanation": "✅ Example in AI-102 lab shows this loop :contentReference[oaicite:27]{index=27}."
        },
        {
          "question": "True or False: You can analyze GIFs of more than one frame.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ Only single-frame GIF is supported :contentReference[oaicite:28]{index=28}."
        },
        {
          "question": "Which service should be used for handwritten document OCR instead?",
          "options": [
            "Document Intelligence Read model",
            "Vision Read",
            "Text Analytics OCR",
            "Form Recognizer"
          ],
          "answer": "Document Intelligence Read model",
          "type": "hard",
          "explanation": "✅ Doc Intelligence Read is optimized for document OCR :contentReference[oaicite:29]{index=29}."
        }
      ]
    },
    {
      "name": "Unit 2: Read text in images",
      "questions": [
        {
          "question": "What capability does the Computer Vision Read API provide?",
          "options": [
            "Extracts printed and handwritten text from images",
            "Translates text",
            "Performs sentiment analysis",
            "Generates image captions"
          ],
          "answer": "Extracts printed and handwritten text from images",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ The Read API is optimized for OCR on both printed and handwritten text :contentReference[oaicite:1]{index=1}."
        },
        {
          "question": "Which API edition is synchronous and suited for real-time OCR of images?",
          "options": [
            "OCR (v4.0)",
            "Document Read (asynchronous)",
            "Translation API",
            "Face API"
          ],
          "answer": "OCR (v4.0)",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ OCR v4.0 is synchronous and optimized for in‑the‑wild images :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "Which option is asynchronous and designed for document-heavy use?",
          "options": [
            "Document Read model",
            "OCR v4.0",
            "Text Analytics",
            "Speech-to-Text"
          ],
          "answer": "Document Read model",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Document Read asynchronously handles multi-page PDFs and scanned docs :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "True or False: The Read API can detect multiple languages in one image automatically.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ By default it extracts all visible text without requiring a language code :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "Which Python method submits an image for OCR?",
          "options": [
            "client.begin_read()",
            "client.read_in_stream()",
            "client.submit_ocr()",
            "client.extract_text()"
          ],
          "answer": "client.begin_read()",
          "type": "hard",
          "explanation": "✅ Python SDK uses `begin_read()` to start the OCR operation :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "How do you retrieve OCR results after submission?",
          "options": [
            "poll the returned operation object",
            "call client.get_text()",
            "await client.read()",
            "use synchronous method only"
          ],
          "answer": "poll the returned operation object",
          "type": "medium",
          "explanation": "✅ You get an LROPoller from `begin_read()` and poll until done :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "What is a typical operation suffix in REST for Read API?",
          "options": ["/read/analyze", "/ocr", "/extract", "/vision/read"],
          "answer": "/vision/v3.2/read/analyze",
          "type": "hard",
          "explanation": "✅ This endpoint is used for Read v3.2 OCR operations :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "Which parameter can force a specific language during OCR?",
          "options": ["language", "lang", "locale", "textLang"],
          "answer": "language",
          "type": "medium",
          "explanation": "✅ You can set the `language` query to focus OCR on one language :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "What is the default reading order for Latin languages?",
          "options": [
            "Left-to-right, top-to-bottom",
            "Right-to-left",
            "Bottom-to-top",
            "Random"
          ],
          "answer": "Left-to-right, top-to-bottom",
          "type": "medium",
          "explanation": "✅ That is the default reading order when language unset :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which extra parameter enables 'natural' reading order?",
          "options": [
            "readingOrder=natural",
            "orderMode=natural",
            "layout=natural",
            "orderType=natural"
          ],
          "answer": "readingOrder=natural",
          "type": "hard",
          "explanation": "✅ Use `readingOrder=natural` for more human-like ordering :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "Scenario: You only need text from the middle pages of a PDF. How?",
          "options": [
            "Set pages=3-5",
            "Pass page range in body",
            "Extract PDF manually",
            "OCR can't do page filters"
          ],
          "answer": "Set pages=3-5",
          "type": "medium",
          "explanation": "✅ Use the `pages` query parameter to specify pages :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "Which visual feature is NOT provided by the Read API?",
          "options": [
            "BoundingBoxes",
            "Text content",
            "Font color",
            "Confidence score"
          ],
          "answer": "Font color",
          "type": "medium",
          "explanation": "✅ OCR returns text, positions, confidence, not color info."
        },
        {
          "question": "What object holds the bounding polygon of a text line in JSON?",
          "options": ["boundingPolygon", "rectangle", "poly", "coords"],
          "answer": "boundingPolygon",
          "type": "hard",
          "explanation": "✅ Lines include `boundingPolygon` with coordinate arrays :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "Which model-version can be specified in Read API?",
          "options": ["2022-04-30", "2020-01-01", "2023-07-15", "2021-01-01"],
          "answer": "2022-04-30",
          "type": "hard",
          "explanation": "✅ Model-version 2022‑04‑30 is documented in Read v3.2 API :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "True or False: Handwritten text is supported in English only.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Handwritten text support is currently English-only :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "Which SDK exception arises on bad key?",
          "options": [
            "HttpResponseError",
            "KeyError",
            "UnauthorizedError",
            "ValueError"
          ],
          "answer": "HttpResponseError",
          "type": "hard",
          "explanation": "✅ Python SDK throws HttpResponseError for HTTP 401/403 :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Why might OCR miss small font text?",
          "options": [
            "Image resolution too low",
            "Wrong language",
            "Key expired",
            "OCR only reads large text"
          ],
          "answer": "Image resolution too low",
          "type": "medium",
          "explanation": "✅ Small or low-res text may not be detected by OCR :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "Coding: Which pip package is used for Read API in Python?",
          "options": [
            "azure-ai-vision",
            "azure-cognitiveservices-vision-computervision",
            "azure-ai-textanalytics",
            "azure-computer-vision"
          ],
          "answer": "azure-cognitiveservices-vision-computervision",
          "type": "medium",
          "explanation": "✅ The quickstart uses this package :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "Scenario: You run Read API in browser studio—what do you do?",
          "options": [
            "Upload image file or URL",
            "Use REST client",
            "Use CLI only",
            "Need Container"
          ],
          "answer": "Upload image file or URL",
          "type": "medium",
          "explanation": "✅ Studio allows file upload or URL input :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "True or False: Read API can extract handwritten text in languages beyond English.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ Handwriting beyond English is not yet supported :contentReference[oaicite:19]{index=19}."
        },
        {
          "question": "Which parameter ensures correct text flow in Latin languages?",
          "options": [
            "readingOrder=natural",
            "flowOrder=true",
            "order=native",
            "layout=flow"
          ],
          "answer": "readingOrder=natural",
          "type": "medium",
          "explanation": "✅ Human-friendly reading order uses that param :contentReference[oaicite:20]{index=20}."
        },
        {
          "question": "Module assessment: True or False—You should wait for completion before accessing OCR results.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Must poll operation until status == succeeded before reading :contentReference[oaicite:21]{index=21}."
        },
        {
          "question": "Which Python method reads from a local image file?",
          "options": [
            "client.begin_read_in_stream()",
            "client.begin_read_from_file()",
            "client.read_local()",
            "client.analyze_file()"
          ],
          "answer": "client.begin_read_in_stream()",
          "type": "hard",
          "explanation": "✅ `begin_read_in_stream()` processes local image data :contentReference[oaicite:22]{index=22}."
        },
        {
          "question": "Scenario: You want to archive OCR results as JSON. What to use?",
          "options": [
            "Save raw operation result",
            "Copy from console logs",
            "Screenshot output",
            "OCR doesn’t support JSON"
          ],
          "answer": "Save raw operation result",
          "type": "medium",
          "explanation": "✅ Use the operation result's `.as_dict()` or JSON output programmatically."
        },
        {
          "question": "Which scenario calls for Document Read API instead of OCR v4.0?",
          "options": [
            "Large PDFs with tables",
            "Single label sign on image",
            "Real-time signage capture",
            "Face detection"
          ],
          "answer": "Large PDFs with tables",
          "type": "hard",
          "explanation": "✅ Document Read handles structured layouts and large documents :contentReference[oaicite:23]{index=23}."
        },
        {
          "question": "Why is OCR called 'universal script-based'?",
          "options": [
            "Supports multiple writing systems",
            "Uses a single language only",
            "Detects only Latin",
            "Requires script input"
          ],
          "answer": "Supports multiple writing systems",
          "type": "medium",
          "explanation": "✅ The engine supports many scripts for global OCR :contentReference[oaicite:24]{index=24}."
        }
      ]
    },
    {
      "name": "Unit 3: Detect, Analyze & Recognize Faces",
      "questions": [
        {
          "question": "What capabilities does the Azure AI Face service offer? (Select 3)",
          "options": [
            "Face detection & analysis",
            "Face verification & identification",
            "Liveness detection",
            "Image translation"
          ],
          "answer": [
            "Face detection & analysis",
            "Face verification & identification",
            "Liveness detection"
          ],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ Face service supports detection, analysis, verification, ID, and liveness :contentReference[oaicite:1]{index=1}. Image translation is unrelated."
        },
        {
          "question": "True or False: Face detection must be done before verification or identification.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Detection provides face ID and bounding boxes needed for later verification and identification :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "Which method in Python SDK creates the client?",
          "options": [
            "FaceClient(endpoint, credential)",
            "ComputerVisionClient(...)",
            "VisionClient(...)",
            "ImageAnalysisClient(...)"
          ],
          "answer": "FaceClient(endpoint, credential)",
          "type": "medium",
          "explanation": "✅ Use `FaceClient(...)` from `azure.ai.vision.face` :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "Which of the following detects facial landmarks (e.g., eyes, nose)?",
          "options": [
            "Detect with faceLandmarks=True",
            "AnalyzeObjects",
            "Read API",
            "Tag Image"
          ],
          "answer": "Detect with faceLandmarks=True",
          "type": "medium",
          "explanation": "✅ Landmark detection is part of face detection with appropriate flags :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "What shape describes location of a face in JSON?",
          "options": [
            "faceRectangle",
            "boundingBox",
            "locationPolygon",
            "faceCoords"
          ],
          "answer": "faceRectangle",
          "type": "easy",
          "explanation": "✅ JSON returns `faceRectangle` with left, top, width, height :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Which attributes were recently restricted due to Responsible AI? (Select 2)",
          "options": ["Emotion", "Age", "Hair color", "Liveness"],
          "answer": ["Emotion", "Hair color"],
          "type": "hard",
          "explanation": "✅ Emotion and some appearance attributes are limited by policy :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "Scenario: You received an 'UnsupportedFeature' error when requesting verification. Likely cause?",
          "options": [
            "Liveness not enabled",
            "Your account lacks verification access",
            "Invalid image format",
            "Insufficient endpoint region"
          ],
          "answer": "Your account lacks verification access",
          "type": "hard",
          "explanation": "✅ Verification/ID require limited access; non-eligible accounts will error :contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "Which API supports 'find similar' faces?",
          "options": [
            "FaceClient.find_similar",
            "VisionClient.compare_faces",
            "ComputerVisionClient.findFaces",
            "ImageAnalysisClient.getMatches"
          ],
          "answer": "FaceClient.find_similar",
          "type": "medium",
          "explanation": "✅ Correct method for similar-face search :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "True or False: Face identification is 'one-to-many' matching.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Identification compares against many in a group while verification is 1-to-1 :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Which structure groups multiple face templates for identity queries?",
          "options": [
            "LargePersonGroup",
            "FaceCollection",
            "FaceSet",
            "GroupLibrary"
          ],
          "answer": "LargePersonGroup",
          "type": "hard",
          "explanation": "✅ Face service uses `LargePersonGroup` for identification tasks :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "Coding: To verify two faces, which method is used?",
          "options": [
            "face_client.verify_face_to_face()",
            "face_client.verify_face_to_person()",
            "face_client.verify",
            "face_client.check_identity()"
          ],
          "answer": "face_client.verify_face_to_face()",
          "type": "hard",
          "explanation": "✅ Use `verify_face_to_face` for 1-to-1 comparison."
        },
        {
          "question": "Which feature helps prevent spoofing using photos or videos?",
          "options": [
            "Liveness detection",
            "Face grouping",
            "Face landmarks",
            "Face attributes"
          ],
          "answer": "Liveness detection",
          "type": "medium",
          "explanation": "✅ Liveness confirms presence of a real person :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "True or False: Face API returns a 'faceId' only if you request it.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ faceId returned when appropriate detection flags are enabled."
        },
        {
          "question": "What does responsible AI guidance warn against with face attributes?",
          "options": [
            "Avoid inferring sensitive traits",
            "Only use high-resolution images",
            "Store faceTemplates indefinitely",
            "Use only for marketing"
          ],
          "answer": "Avoid inferring sensitive traits",
          "type": "medium",
          "explanation": "✅ Responsible guidance discourages gender/ethnicity inference :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "Scenario: You want more accurate gaze tracking. What helps?",
          "options": [
            "Use Detection_03 model for landmarks",
            "Use generic thumbnail detection",
            "Enable emotion analysis",
            "Use OCR"
          ],
          "answer": "Use Detection_03 model for landmarks",
          "type": "hard",
          "explanation": "✅ Detection_03 gives more accurate landmarks needed for gaze :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "Which data structure stores individual faces within a person group?",
          "options": [
            "Person within LargePersonGroup",
            "FaceListItem",
            "FaceReference",
            "PersonFace"
          ],
          "answer": "Person within LargePersonGroup",
          "type": "hard",
          "explanation": "✅ Person group holds Person objects containing face data :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "True or False: Face detection returns results in size-descending order.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Faces are returned sorted by size descending :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Which method trains face group for identification?",
          "options": [
            "begin_train()",
            "train_group()",
            "start_identify_training()",
            "model.train()"
          ],
          "answer": "begin_train()",
          "type": "hard",
          "explanation": "✅ Training for identification uses long-running operation `begin_train()` :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "Why use FaceSessionClient?",
          "options": [
            "For liveness detection sessions",
            "To batch detect faces",
            "To identify celebrities",
            "To analyze image text"
          ],
          "answer": "For liveness detection sessions",
          "type": "medium",
          "explanation": "✅ FaceSessionClient manages liveness detection flows :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "Scenario: You need to find duplicates in a large dataset. Which feature?",
          "options": [
            "Find similar faces",
            "Face verification",
            "Analysis only",
            "Text recognition"
          ],
          "answer": "Find similar faces",
          "type": "medium",
          "explanation": "✅ `find_similar` is used for de-duplication tasks :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "What guideline is recommended for facial recognition privacy?",
          "options": [
            "Obtain consent and delete biometric data when done",
            "Store indefinitely for performance",
            "Use only in background apps",
            "No logged auditing needed"
          ],
          "answer": "Obtain consent and delete biometric data when done",
          "type": "hard",
          "explanation": "✅ Responsible AI requires consent and data retention policies :contentReference[oaicite:19]{index=19}."
        },
        {
          "question": "Which data type is returned for face landmarks in Python SDK?",
          "options": [
            "FaceLandmarks",
            "LandmarkArray",
            "facial_points",
            "LandmarkCoordinates"
          ],
          "answer": "FaceLandmarks",
          "type": "hard",
          "explanation": "✅ Python returns a `FaceLandmarks` object :contentReference[oaicite:20]{index=20}."
        },
        {
          "question": "True or False: You can only use one person group per FaceClient instance.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ FaceClient can access multiple person groups."
        },
        {
          "question": "Which call returns emotion attributes if permitted?",
          "options": [
            "face_client.detect with attributes=['emotion']",
            "face_client.analyze_emotion",
            "computer_vision.detect_emotion",
            "face_client.read_emotion"
          ],
          "answer": "face_client.detect with attributes=['emotion']",
          "type": "hard",
          "explanation": "✅ Emotion attribute can be requested via `detect` attachments when allowed :contentReference[oaicite:21]{index=21}."
        },
        {
          "question": "Scenario: You receive empty faces array for clear image. Likely cause?",
          "options": [
            "No limited access approval granted",
            "Incorrect image URL",
            "Network timeout",
            "Wrong model version"
          ],
          "answer": "No limited access approval granted",
          "type": "medium",
          "explanation": "✅ Face detection is restricted until access is granted :contentReference[oaicite:22]{index=22}."
        },
        {
          "question": "Which Visual Studio code snippet imports the FaceClient namespace in Python?",
          "options": [
            "from azure.ai.vision.face import FaceClient",
            "import FaceClient",
            "from azure.cognitiveservices.face import FaceClient",
            "import azure.face.client"
          ],
          "answer": "from azure.ai.vision.face import FaceClient",
          "type": "medium",
          "explanation": "✅ Import matches preview face SDK :contentReference[oaicite:23]{index=23}."
        },
        {
          "question": "True or False: FaceClient identifies min confidence of 0.7 by default.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "hard",
          "explanation": "✅ Confidence threshold must be specified; no default set automatically."
        }
      ]
    },
    {
      "name": "Unit 4: Classify images",
      "questions": [
        {
          "question": "What does Azure Custom Vision allow you to do?",
          "options": [
            "Detect objects in video streams",
            "Train and deploy custom image classifiers",
            "Translate text in images",
            "Perform face recognition"
          ],
          "answer": "Train and deploy custom image classifiers",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Custom Vision enables creation of tailored image classification models :contentReference[oaicite:1]{index=1}. Others are unrelated tasks."
        },
        {
          "question": "Which classification types are supported? (Select 2)",
          "options": [
            "Multiclass",
            "Multilabel",
            "Binary only",
            "Hierarchical"
          ],
          "answer": ["Multiclass", "Multilabel"],
          "type": "multiple",
          "difficulty": "medium",
          "explanation": "✅ Supports multiclass (one tag per image) and multilabel (multiple tags) :contentReference[oaicite:2]{index=2}."
        },
        {
          "question": "Scenario: You need to tag an image with 'apple' and 'fresh'. Which type should you choose?",
          "options": [
            "Multiclass",
            "Multilabel",
            "Hierarchical",
            "Single label"
          ],
          "answer": "Multilabel",
          "type": "medium",
          "explanation": "✅ Multilabel enables multiple tags per image :contentReference[oaicite:3]{index=3}."
        },
        {
          "question": "Which domain is best for food images?",
          "options": ["Retail", "Food", "Generic", "Landmarks"],
          "answer": "Food",
          "type": "medium",
          "explanation": "✅ The Food domain optimizes for dishes and ingredients :contentReference[oaicite:4]{index=4}."
        },
        {
          "question": "True or False: You can change the classification type after project creation.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ You can update between multicl/class after creation :contentReference[oaicite:5]{index=5}."
        },
        {
          "question": "Approximately how many images per tag are recommended to start?",
          "options": ["50", "10", "500", "1000"],
          "answer": "50",
          "type": "medium",
          "explanation": "✅ Docs suggest ~50 images/tag to bootstrap model :contentReference[oaicite:6]{index=6}."
        },
        {
          "question": "Coding: Which client class is used to train a model in Python?",
          "options": [
            "CustomVisionTrainingClient",
            "VisionClient",
            "ImageClassifierClient",
            "TransportTrainingClient"
          ],
          "answer": "CustomVisionTrainingClient",
          "type": "hard",
          "explanation": "✅ Python quickstart uses `CustomVisionTrainingClient(...)`:contentReference[oaicite:7]{index=7}."
        },
        {
          "question": "Which authentication is required to publish a model for prediction?",
          "options": [
            "Prediction resource key",
            "Subscription key",
            "Admin-level Azure login",
            "No authentication"
          ],
          "answer": "Prediction resource key",
          "type": "medium",
          "explanation": "✅ Prediction API access uses prediction resource key :contentReference[oaicite:8]{index=8}."
        },
        {
          "question": "What does 'publish iteration' do?",
          "options": [
            "Makes model available via REST endpoint",
            "Exports model to ONNX",
            "Saves images",
            "Deletes training data"
          ],
          "answer": "Makes model available via REST endpoint",
          "type": "medium",
          "explanation": "✅ Publishing makes the trained iteration accessible for predictions :contentReference[oaicite:9]{index=9}."
        },
        {
          "question": "Scenario: You want to test your model programmatically after training. Which SDK is used?",
          "options": [
            "CustomVisionPredictionClient",
            "CustomVisionTrainingClient",
            "ImageAnalysisClient",
            "ComputerVisionClient"
          ],
          "answer": "CustomVisionPredictionClient",
          "type": "hard",
          "explanation": "✅ Prediction client is used for evaluating models :contentReference[oaicite:10]{index=10}."
        },
        {
          "question": "Which method performs single-image classification?",
          "options": [
            "predict_image",
            "classify_image",
            "analyze_class",
            "detect_labels"
          ],
          "answer": "classify_image",
          "type": "hard",
          "explanation": "✅ Example uses `predictor.classify_image(...)` :contentReference[oaicite:11]{index=11}."
        },
        {
          "question": "True or False: Confidence scores are shown as fractions between 0 and 1.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Probabilities returned are normalized between 0 and 1 :contentReference[oaicite:12]{index=12}."
        },
        {
          "question": "What is a threshold used for?",
          "options": [
            "Filter predictions by confidence",
            "Set learning rate",
            "Adjust image size",
            "Increase storage"
          ],
          "answer": "Filter predictions by confidence",
          "type": "medium",
          "explanation": "✅ You can set a confidence threshold to ignore low-confidence labels :contentReference[oaicite:13]{index=13}."
        },
        {
          "question": "Which formats can the interface export models to? (Select 2)",
          "options": ["ONNX", "CoreML", "CSV", "MP3"],
          "answer": ["ONNX", "CoreML"],
          "type": "medium",
          "explanation": "✅ Custom Vision supports ONNX and CoreML exports :contentReference[oaicite:14]{index=14}."
        },
        {
          "question": "Which pricing tier offers free trials?",
          "options": ["F0", "S1", "P0", "B1"],
          "answer": "F0",
          "type": "ease",
          "explanation": "✅ F0 is the free tier for Custom Vision :contentReference[oaicite:15]{index=15}."
        },
        {
          "question": "Scenario: You want classification offline, what should you do?",
          "options": [
            "Export model to container",
            "Use OCR",
            "Use Face API",
            "Use Computer Vision"
          ],
          "answer": "Export model to container",
          "type": "medium",
          "explanation": "✅ Exporting enables edge deployment via containers :contentReference[oaicite:16]{index=16}."
        },
        {
          "question": "What is the purpose of tag images?",
          "options": [
            "Provide labeled data for training",
            "Generate thumbnails",
            "Compress images",
            "Translate image text"
          ],
          "answer": "Provide labeled data for training",
          "type": "medium",
          "explanation": "✅ Tagging teaches the model what to identify :contentReference[oaicite:17]{index=17}."
        },
        {
          "question": "True or False: You can train a model with fewer than 10 images per tag.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ Documentation suggests at least ~50 images/tag; lower counts reduce performance."
        },
        {
          "question": "Which metric measures model's ability to differentiate between classes?",
          "options": [
            "Precision and recall",
            "Frame rate",
            "Latency",
            "Storage usage"
          ],
          "answer": "Precision and recall",
          "type": "hard",
          "explanation": "✅ These are used in the performance metrics dashboard :contentReference[oaicite:18]{index=18}."
        },
        {
          "question": "Which language SDK supports both training and prediction? (Select 2)",
          "options": ["Python", "Go", "JavaScript", "C#"],
          "answer": ["Python", "C#"],
          "type": "medium",
          "explanation": "✅ Both Python and C# have full SDK support :contentReference[oaicite:19]{index=19}."
        },
        {
          "question": "Scenario: Your predictions are low confidence—what should you do?",
          "options": [
            "Add more labeled images",
            "Increase endpoint tier",
            "Use OCR instead",
            "Resize images to smaller size"
          ],
          "answer": "Add more labeled images",
          "type": "medium",
          "explanation": "✅ More data helps improve accuracy :contentReference[oaicite:20]{index=20}."
        },
        {
          "question": "Coding: Create a tag named 'Cat' in C# — which method?",
          "options": [
            "trainer.CreateTag(project.Id, \"Cat\")",
            "trainer.AddLabel(\"Cat\")",
            "predictor.CreateTag(\"Cat\")",
            "client.NewTag(\"Cat\")"
          ],
          "answer": "trainer.CreateTag(project.Id, \"Cat\")",
          "type": "hard",
          "explanation": "✅ Code snippet shows tag creation via `CreateTag(...)` :contentReference[oaicite:21]{index=21}."
        },
        {
          "question": "True or False: The Custom Vision portal lets you delete projects.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "ease",
          "explanation": "✅ Projects can be deleted via portal :contentReference[oaicite:22]{index=22}."
        },
        {
          "question": "Which Visual Studio Code extension can speed up lab work?",
          "options": [
            "Azure IoT Tools",
            "Azure Custom Vision",
            "Python",
            "Live Share"
          ],
          "answer": "Azure Custom Vision",
          "type": "medium",
          "explanation": "✅ There is a Custom Vision extension to streamline workflows."
        },
        {
          "question": "In module assessment: True or False—You need both training and prediction resources.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Training and prediction each require their own resource :contentReference[oaicite:23]{index=23}."
        },
        {
          "question": "What is 'active learning' in Custom Vision?",
          "options": [
            "Reviewing auto-tagged images to improve model",
            "Adding new visual features",
            "Applying OCR on tags",
            "Automated checking of images"
          ],
          "answer": "Reviewing auto-tagged images to improve model",
          "type": "hard",
          "explanation": "✅ Custom Vision suggests images for review to enhance model :contentReference[oaicite:24]{index=24}."
        }
      ]
    },
    {
      "name": "Unit 5: Detect objects in images",
      "questions": [
        {
          "question": "What is the main purpose of object detection in Azure Custom Vision?",
          "options": [
            "Classify entire image into one label",
            "Locate and classify multiple objects within an image",
            "Detect text in images",
            "Analyze face attributes"
          ],
          "answer": "Locate and classify multiple objects within an image",
          "type": "single",
          "difficulty": "medium",
          "explanation": "✅ Object detection identifies and localizes multiple objects, unlike image classification which labels the full image."
        },
        {
          "question": "Which annotation is required for object detection training?",
          "options": [
            "Bounding box + label",
            "Single tag per image",
            "Image caption",
            "None"
          ],
          "answer": "Bounding box + label",
          "type": "medium",
          "explanation": "✅ Object detection requires bounding boxes with labels for each object in training images."
        },
        {
          "question": "What type of Custom Vision project should you create for object detection?",
          "options": [
            "Object detection",
            "Classification",
            "Segmentation",
            "OCR"
          ],
          "answer": "Object detection",
          "type": "easy",
          "explanation": "✅ You must select the object detection project type to enable bounding box training."
        },
        {
          "question": "Which domain optimizes for traffic-related objects?",
          "options": ["None (General)", "Retail", "Landmarks", "Food"],
          "answer": "None (General)",
          "type": "medium",
          "explanation": "✅ The general ('none') domain handles diverse objects; others serve specific use cases."
        },
        {
          "question": "True or False: You can mix tagged and untagged images in training.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "❌ Only fully tagged images with bounding boxes should be used in object detection training."
        },
        {
          "question": "Approximately how many images per object class are recommended?",
          "options": ["50", "10", "500", "1000"],
          "answer": "50",
          "type": "medium",
          "explanation": "✅ Microsoft recommends starting with ~50 images per object class to bootstrap detection performance."
        },
        {
          "question": "Scenario: You want to detect small objects that are hard to see. What should you do?",
          "options": [
            "Use close-up images and annotate accurately",
            "Reduce image size",
            "Use only classification project",
            "Use text captions instead"
          ],
          "answer": "Use close-up images and annotate accurately",
          "type": "medium",
          "explanation": "✅ Detecting small objects requires higher resolution images and precise bounding boxes."
        },
        {
          "question": "Which Python SDK client is used for training object detection?",
          "options": [
            "CustomVisionTrainingClient",
            "ObjectDetectionTrainer",
            "VisionClassifier",
            "ImageAnalysisClient"
          ],
          "answer": "CustomVisionTrainingClient",
          "type": "hard",
          "explanation": "✅ The training client `CustomVisionTrainingClient` is used for both classification and object detection."
        },
        {
          "question": "Which method publishes a trained object detection model?",
          "options": [
            "client.publish_iteration",
            "client.deploy_model",
            "client.publish_model",
            "client.create_export"
          ],
          "answer": "client.publish_iteration",
          "type": "hard",
          "explanation": "✅ You publish an iteration of a Custom Vision project using `publish_iteration`."
        },
        {
          "question": "True or False: Once an object detection model is published, you can get predictions with bounding boxes.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Published models return both object labels and bounding boxes in predictions."
        },
        {
          "question": "Scenario: You want the model to detect whether images contain a helmet and safety vest together. What project type and labeling approach?",
          "options": [
            "Object detection – bounding boxes per item",
            "Classification – tags for helmet and vest",
            "Classification – combined tag",
            "OCR – text on vest"
          ],
          "answer": "Object detection – bounding boxes per item",
          "type": "medium",
          "explanation": "✅ Object detection with separate bounding boxes allows identifying both items in one image."
        },
        {
          "question": "Which Python SDK client is used for getting prediction on images?",
          "options": [
            "CustomVisionPredictionClient",
            "CustomVisionTrainingClient",
            "VisionPredictionClient",
            "DetectionClient"
          ],
          "answer": "CustomVisionPredictionClient",
          "type": "hard",
          "explanation": "✅ Prediction client `CustomVisionPredictionClient` is appropriate for post-training inference."
        },
        {
          "question": "What parameter is required to specify the published model version?",
          "options": [
            "iteration_name",
            "model_version",
            "published_iteration_name",
            "model_id"
          ],
          "answer": "published_iteration_name",
          "type": "hard",
          "explanation": "✅ The SDK requires the `published_iteration_name` to use the correct model endpoint."
        },
        {
          "question": "Which property in prediction response holds bounding box data?",
          "options": ["bounding_box", "box", "rectangle", "region"],
          "answer": "bounding_box",
          "type": "medium",
          "explanation": "✅ The JSON response includes a `bounding_box` field describing the object's location."
        },
        {
          "question": "True or False: You must export object detection models to use them offline.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ For offline or edge deployment, you must export to TensorFlow, ONNX, or CoreML container."
        },
        {
          "question": "Which export formats are supported? (Select 2)",
          "options": ["TensorFlow", "ONNX", "CSV", "MP3"],
          "answer": ["TensorFlow", "ONNX"],
          "type": "medium",
          "explanation": "✅ Custom Vision supports TensorFlow and ONNX exports for object detection models."
        },
        {
          "question": "Module assessment: True or False—You need separate training and prediction resources.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ Custom Vision requires dedicated training and prediction resources to function."
        },
        {
          "question": "Scenario: You observe bounding boxes are duplicated heavily. What should you check?",
          "options": [
            "Set Non-Max Suppression correctly in prediction threshold",
            "Reduce dataset size",
            "Change annotation format",
            "Switch to classification"
          ],
          "answer": "Set Non-Max Suppression correctly in prediction threshold",
          "type": "hard",
          "explanation": "✅ Proper suppression of duplicate boxes relies on threshold adjustments in detection settings."
        },
        {
          "question": "Which metric is used to evaluate object detection accuracy?",
          "options": [
            "Mean average precision (mAP)",
            "F1-score per image",
            "ROC‑AUC",
            "BLEU score"
          ],
          "answer": "Mean average precision (mAP)",
          "type": "hard",
          "explanation": "✅ mAP is a standard metric used to measure detection performance over IoU thresholds."
        },
        {
          "question": "Coding: To create a new object tag called 'Car', which method is used?",
          "options": [
            "trainer.create_tag(project.id, 'Car')",
            "trainer.create_object_tag('Car')",
            "trainer.add_tag('Car')",
            "trainer.new_class('Car')"
          ],
          "answer": "trainer.create_tag(project.id, 'Car')",
          "type": "hard",
          "explanation": "✅ Tag creation is done via `create_tag()` with project ID and name parameters."
        },
        {
          "question": "True or False: You can add tags to images after model is published.",
          "options": ["True", "False"],
          "answer": "True",
          "type": "medium",
          "explanation": "✅ You can continue labeling and retraining new iterations post-publish."
        },
        {
          "question": "Scenario: You need to detect rotated objects. What advice is best?",
          "options": [
            "Include rotated versions in training and annotate all",
            "Only include upright objects",
            "Use only classification",
            "Use OCR to detect orientation"
          ],
          "answer": "Include rotated versions in training and annotate all",
          "type": "medium",
          "explanation": "✅ Training data should reflect object orientation variance to improve detection."
        },
        {
          "question": "Which method triggers model training?",
          "options": [
            "trainer.train_project(project.id)",
            "trainer.start_training(project)",
            "trainer.begin_training()",
            "trainer.create_iteration()"
          ],
          "answer": "trainer.train_project(project.id)",
          "type": "hard",
          "explanation": "✅ Training is initiated using `train_project()` in the training client."
        },
        {
          "question": "Why would you set a lower confidence threshold in predictions?",
          "options": [
            "To capture more potential objects (with more false positives)",
            "To reduce false positives",
            "To speed up inference",
            "To lower memory usage"
          ],
          "answer": "To capture more potential objects (with more false positives)",
          "type": "medium",
          "explanation": "✅ Lower threshold results in more detections but potentially more incorrect ones."
        },
        {
          "question": "Which Visual Studio Code extension supports Custom Vision workflows?",
          "options": [
            "Azure Custom Vision",
            "Python",
            "Azure Storage",
            "Live Share"
          ],
          "answer": "Azure Custom Vision",
          "type": "medium",
          "explanation": "✅ The Azure Custom Vision extension simplifies dataset labeling and retraining."
        },
        {
          "question": "True or False: You need bounding boxes in inference calls to get localization.",
          "options": ["True", "False"],
          "answer": "False",
          "type": "medium",
          "explanation": "✅ The model returns bounding boxes in inference results automatically without annotation."
        },
        {
          "question": "Scenario: You want to move object detection to edge device. What’s required?",
          "options": [
            "Export model and deploy container",
            "Only reuse training resource",
            "Switch to classification",
            "Use desktop-only APIs"
          ],
          "answer": "Export model and deploy container",
          "type": "medium",
          "explanation": "✅ Edge deployment requires exported model and container runtime."
        },
        {
          "question": "What improvement leverages 'active learning' in object detection?",
          "options": [
            "Reviewing and labeling suggested images to refine detection",
            "Adjusting bounding boxes automatically",
            "Translating labels to other languages",
            "Compressing images for faster upload"
          ],
          "answer": "Reviewing and labeling suggested images to refine detection",
          "type": "hard",
          "explanation": "✅ Active learning suggests ambiguous detections for review to improve model iteratively."
        }
      ]
    }
  ]
}
